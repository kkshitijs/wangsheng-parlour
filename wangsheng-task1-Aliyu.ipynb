{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba5b0",
   "metadata": {},
   "source": [
    "# Task 1: Housing Price Regression\n",
    "\n",
    "Motivation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility of notebook\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "GB_LEARNING_RATE = 0.1\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20254, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8c943",
   "metadata": {},
   "source": [
    "# Improving our Dataset with Aux Data\n",
    "\n",
    "Since we have auxilliary data, we can add them to our training data to see if our model improves. We can find the number of infrasructures close to our listings. The infrastructures we have include:\n",
    "* commercial centers\n",
    "* mrt stations\n",
    "* primary schools\n",
    "* secondary schools\n",
    "* shopping malls\n",
    "\n",
    "We also have data for \n",
    "* subzones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6808e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcitons\n",
    "# first we define functions we need\n",
    "# haversine distance function\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "Replicating the same formula as mentioned in Wiki\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lat1, long1, lat2, long2 = map(radians, [lat1, long1, lat2, long2])\n",
    "    # haversine formula \n",
    "    dlon = long2 - long1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "# find the nmber of nearest infrastructure to our property listing\n",
    "# km is the maximum distance we want to calculate from. \n",
    "# calculate the number of infrastructure within x km\n",
    "def count_nearest(lat, long, infrastructure, km):\n",
    "    distances = infrastructure.apply(\n",
    "        lambda row: haversine(lat, long, row['lat'], row['lng']), \n",
    "        axis=1)\n",
    "    \n",
    "    return sum(i <= km for i in distances)\n",
    "\n",
    "\n",
    "# find the distance of the nearest infrastructure to our property listing\n",
    "def find_nearest_distance(lat, long, infrastructure):\n",
    "    distances = infrastructure.apply(\n",
    "        lambda row: haversine(lat, long, row['lat'], row['lng']), \n",
    "        axis=1)\n",
    "    return distances.min()\n",
    "\n",
    "\n",
    "# get the population of the subzone the listing is in\n",
    "def find_subzone_population(subzone, subzones_df):\n",
    "    sz = subzones_df\n",
    "    if subzone == \"\":\n",
    "        sz_pop = None\n",
    "    else:\n",
    "        sz_pop = sz.loc[sz['name'] == subzone, 'population'].item()\n",
    "    return sz_pop\n",
    "\n",
    "# get the population density of the subzone the listing is in\n",
    "def find_subzone_population_density(subzone, subzones_df):\n",
    "    sz = subzones_df\n",
    "    if subzone == \"\":\n",
    "        sz_pop_density = None\n",
    "    else:\n",
    "        sz_pop_density = sz.loc[sz['name'] == subzone, 'population'].item()/sz.loc[sz['name'] == subzone, 'area_size'].item()\n",
    "    return sz_pop_density\n",
    "\n",
    "def improve_dataset(df):\n",
    "    df = df.copy()\n",
    "    # get the files containing the infrastructures data\n",
    "    cc = pd.read_csv(\"data/auxiliary-data/sg-commerical-centres.csv\")\n",
    "    mrt = pd.read_csv(\"data/auxiliary-data/sg-mrt-stations.csv\")\n",
    "    ps = pd.read_csv(\"data/auxiliary-data/sg-primary-schools.csv\")\n",
    "    ss = pd.read_csv(\"data/auxiliary-data/sg-secondary-schools.csv\")\n",
    "    sm = pd.read_csv(\"data/auxiliary-data/sg-shopping-malls.csv\")\n",
    "    sz = pd.read_csv(\"data/auxiliary-data/sg-subzones.csv\")\n",
    "    \n",
    "    df.subzone = df.subzone.fillna('') \n",
    "    \n",
    "    # get the population of the subzone the listing is in, and add to data\n",
    "    df['subzone_pop'] = df.apply(\n",
    "        lambda row: find_subzone_population(row['subzone'], sz), \n",
    "        axis=1)\n",
    "    df['subzone_pop'] = df['subzone_pop'].round(decimals=3)\n",
    "    \n",
    "    # get the population of the subzone the listing is in, and add to data\n",
    "    df['subzone_pop_density'] = df.apply(\n",
    "        lambda row: find_subzone_population(row['subzone'], sz), \n",
    "        axis=1)\n",
    "    df['subzone_pop_density'] = df['subzone_pop_density'].round(decimals=3)\n",
    "    \n",
    "    # calculate distance to nearest commercial center\n",
    "    df['dist_2_nearest_cc'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], cc), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_cc'] = df['dist_2_nearest_cc'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of commercial centers within x km\n",
    "    df['nearest_cc_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], cc, 1), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest mrt station\n",
    "    df['dist_2_nearest_mrt'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], mrt), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_mrt'] = df['dist_2_nearest_mrt'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of mrt stations within x km\n",
    "    df['nearest_mrt_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], mrt, 1), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest primary school\n",
    "    df['dist_2_nearest_ps'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], ps), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_ps'] = df['dist_2_nearest_ps'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of primary schools within x km\n",
    "    df['nearest_ps_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], ps, 1), \n",
    "        axis=1)\n",
    "    # calculate distance to nearest secondary school\n",
    "    df['dist_2_nearest_ss'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], ss), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_ss'] = df['dist_2_nearest_ss'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of secondary schools within x km\n",
    "    df['nearest_ss_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], ss, 1), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest shopping mall\n",
    "    df['dist_2_nearest_sm'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], sm), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_sm'] = df['dist_2_nearest_sm'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of secondary schools within x km\n",
    "    df['nearest_sm_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], sm, 1), \n",
    "        axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae9e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = improve_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c755333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20254, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>title</th>\n",
       "      <th>address</th>\n",
       "      <th>property_name</th>\n",
       "      <th>property_type</th>\n",
       "      <th>tenure</th>\n",
       "      <th>built_year</th>\n",
       "      <th>num_beds</th>\n",
       "      <th>num_baths</th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_2_nearest_cc</th>\n",
       "      <th>nearest_cc_count</th>\n",
       "      <th>dist_2_nearest_mrt</th>\n",
       "      <th>nearest_mrt_count</th>\n",
       "      <th>dist_2_nearest_ps</th>\n",
       "      <th>nearest_ps_count</th>\n",
       "      <th>dist_2_nearest_ss</th>\n",
       "      <th>nearest_ss_count</th>\n",
       "      <th>dist_2_nearest_sm</th>\n",
       "      <th>nearest_sm_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122881</td>\n",
       "      <td>hdb flat for sale in 866 yishun street 81</td>\n",
       "      <td>sembawang / yishun (d27)</td>\n",
       "      <td>866 yishun street 81</td>\n",
       "      <td>hdb 4 rooms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1115</td>\n",
       "      <td>...</td>\n",
       "      <td>3.339</td>\n",
       "      <td>0</td>\n",
       "      <td>0.574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.276</td>\n",
       "      <td>3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>3</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259374</td>\n",
       "      <td>hdb flat for sale in 506b serangoon north aven...</td>\n",
       "      <td>hougang / punggol / sengkang (d19)</td>\n",
       "      <td>hdb-serangoon estate</td>\n",
       "      <td>hdb</td>\n",
       "      <td>99-year leasehold</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>2.402</td>\n",
       "      <td>0</td>\n",
       "      <td>1.734</td>\n",
       "      <td>0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.291</td>\n",
       "      <td>4</td>\n",
       "      <td>0.553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>665422</td>\n",
       "      <td>4 bed condo for sale in meyerhouse</td>\n",
       "      <td>128 meyer road</td>\n",
       "      <td>meyerhouse</td>\n",
       "      <td>condo</td>\n",
       "      <td>freehold</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>...</td>\n",
       "      <td>2.171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id                                              title  \\\n",
       "0      122881          hdb flat for sale in 866 yishun street 81   \n",
       "1      259374  hdb flat for sale in 506b serangoon north aven...   \n",
       "2      665422                 4 bed condo for sale in meyerhouse   \n",
       "\n",
       "                              address         property_name property_type  \\\n",
       "0            sembawang / yishun (d27)  866 yishun street 81   hdb 4 rooms   \n",
       "1  hougang / punggol / sengkang (d19)  hdb-serangoon estate           hdb   \n",
       "2                      128 meyer road            meyerhouse         condo   \n",
       "\n",
       "              tenure  built_year  num_beds  num_baths  size_sqft  ...  \\\n",
       "0                NaN      1988.0       3.0        2.0       1115  ...   \n",
       "1  99-year leasehold      1992.0       4.0        2.0       1575  ...   \n",
       "2           freehold      2022.0       4.0        6.0       3070  ...   \n",
       "\n",
       "  dist_2_nearest_cc nearest_cc_count dist_2_nearest_mrt  nearest_mrt_count  \\\n",
       "0             3.339                0              0.574                  1   \n",
       "1             2.402                0              1.734                  0   \n",
       "2             2.171                0              1.320                  0   \n",
       "\n",
       "  dist_2_nearest_ps  nearest_ps_count  dist_2_nearest_ss  nearest_ss_count  \\\n",
       "0             0.276                 3              0.183                 3   \n",
       "1             0.123                 3              0.291                 4   \n",
       "2             0.891                 1              0.895                 1   \n",
       "\n",
       "  dist_2_nearest_sm nearest_sm_count  \n",
       "0             0.621                1  \n",
       "1             0.553                1  \n",
       "2             0.824                1  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_.shape)\n",
    "df_.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Talk about pre-processing here.\n",
    "Visualize plots of original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100bab",
   "metadata": {},
   "source": [
    "For HDB,\n",
    "\n",
    "Assume that num_beds refers only to the bedrooms excluding the living rooms\n",
    "* Hdb 2-room = 1 bedroom , 1 bathroom\n",
    "* Hdb 3-room = 2 bedroom , 2 bathroom\n",
    "* Hdb 4-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 5-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 3-gen = 4 bedroom , 3 bathroom\n",
    "* Hdb Executive = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Masionette = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Jumbo = 4 bedroom, 4 bathroom\n",
    "\n",
    "References:\n",
    "* http://www.data.com.sg/template-m.jsp?p=my/1.html \n",
    "* https://www.hdb.gov.sg/residential/buying-a-flat/finding-a-flat/types-of-flats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    pass\n",
    "\n",
    "# IMPORTANT: Trivial modifications only. Do not aggregate/standardize/impute here!!\n",
    "\n",
    "def ignore_attributes(df) -> pd.DataFrame:\n",
    "    # Drop listing id; nominal identifier with no meaning\n",
    "    df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "    # Drop elevation; all the values are 0, spurious attribute\n",
    "    df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "    # Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "    df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "    # Drop floor level as 83% missing and sparse with the rest of the values. \n",
    "    # Not enough data available to get the model trained.\n",
    "    df.drop('floor_level', axis=1, inplace=True)\n",
    "\n",
    "    # Drop column property_type, tenure and furnishing as it is now encoded\n",
    "    df.drop('property_type',axis = 1, inplace=True)\n",
    "    df.drop('tenure',axis=1, inplace=True)\n",
    "    df.drop('furnishing', axis=1, inplace=True)\n",
    "\n",
    "    # BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    df.drop('address', axis=1, inplace=True)\n",
    "    df.drop('property_name', axis=1, inplace=True)\n",
    "    df.drop('built_year', axis=1, inplace=True)\n",
    "    df.drop('available_unit_types', axis=1, inplace=True)\n",
    "    df.drop('total_num_units', axis=1, inplace=True) # Bernard verified dropping it. 27.9% missing.\n",
    "    df.drop('lat', axis=1, inplace=True)\n",
    "    df.drop('lng', axis=1, inplace=True)\n",
    "    df.drop('subzone', axis=1, inplace=True)\n",
    "    df.drop('planning_area', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df) -> pd.DataFrame:\n",
    "    # Treat missing year data as new.\n",
    "    # Semantically, we define this attribute as the depreciation factor for pricing.\n",
    "    # A new house or one with missing data denotes the depreciation factor is 0 or unknown.\n",
    "    # The depreciation factor is assumed to be the difference between construction and current year.\n",
    "    # TODO: Maybe do not treat future years as current! Inflation factor might be one to look out for.\n",
    "    df['built_year'] = df['built_year'].fillna(2022)\n",
    "    \n",
    "    # TODO: 80 are missing. Should we remove them or should we keep it as 0?\n",
    "    # Verify assumption if studio qualifies as 1 bed. \n",
    "    # 75 of missing are studio, we replace the Nan as 1\n",
    "    filter_beds_studio = ((df.num_beds.isna()) & ((df.title.str.contains('studio','Studio', flags=re.IGNORECASE, regex=True))))\n",
    "    df.loc[filter_beds_studio, \"num_beds\"] = 1\n",
    "    # 5 of missing, we do not have much info. Use 0 to denote absence of attribute\n",
    "    df['num_beds'] = df['num_beds'].fillna(0)\n",
    "\n",
    "    # TODO: 400 are missing. Cannot remove so many data. Use 0 to denote absence of attribute.\n",
    "    df['num_baths'] = df['num_baths'].fillna(0)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "def handle_invalid_values(df) -> pd.DataFrame:\n",
    "    # Price is the target regression variable. If negative or 0, treat that row as invalid.\n",
    "    if 'price' in df:\n",
    "        df = df[df.price > 0]\n",
    "\n",
    "    # TODO: Verify the steps below for HDB - bed/bath ratio, price checks\n",
    "    # Filtering those with number of bathrooms more than number of bedrooms for HDB\n",
    "    filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "\n",
    "    #Filtering those with number of bathrooms more than 4, number of bedrooms more than 4 for HDB\n",
    "    filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 5)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "    \n",
    "    # price ; filtering for HDB price more than 2 million\n",
    "    if 'price' in df:\n",
    "        filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "        df = df.drop(df[filter_price_hdb].index)\n",
    "        \n",
    "    ## Outliers\n",
    "    \n",
    "    # Filtering those hdb with more than 2000 size_sqft\n",
    "    filter_size_hdb = ((df.size_sqft > 2000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df[filter_size_hdb][['property_type','num_baths','num_beds','size_sqft','price']]\n",
    "    df = df.drop(df[filter_size_hdb].index)\n",
    "    \n",
    "    \n",
    "    if 'price' in df:\n",
    "    # Filtering those data with less than $200/square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_price_sqft_200 = ((df['price per sq ft'] < 200) & (df['price per sq ft'] > 0))\n",
    "        df = df.drop(df[filter_price_sqft_200].index)\n",
    "    # Filtering those data with less than 500 square feet, and more than $5000 per square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_size = ((df['price per sq ft'] > 5000) & (df['price per sq ft'] > 0) & (df['size_sqft'] < 500))\n",
    "        df = df.drop(df[filter_size].index)\n",
    "        \n",
    "    df.drop('price per sq ft', axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def transform_data(df) -> pd.DataFrame:\n",
    "    # TODO: Test against not doing this.\n",
    "    df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "    \n",
    "    # Convert built_year into the aforementioned depreciation factor\n",
    "    df[\"depreciation\"] = (2022-df[\"built_year\"])\n",
    "    \n",
    "    # TODO: Add details on why we are doing so\n",
    "    df.loc[df.property_type.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'property_type'] = 'hdb'\n",
    "    df.loc[df.property_type.str.contains('condo', flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df['property_type'] = df['property_type'].str.lower()\n",
    "    df.loc[df.property_type.str.contains('cluster house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('townhouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('land only', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('apartment',  flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df.loc[df.property_type.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'property_type'] = 'bungalow'\n",
    "    df.loc[df.property_type.str.contains('semi-detached house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('corner terrace',flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('shophouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    df.loc[df.property_type.str.contains('conservation house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    \n",
    "    # Get one hot encoding of columns property_type\n",
    "    property_columns = ['bungalow', 'condo', 'hdb', 'corner', 'landed', 'protected', 'terraced house', 'walk-up']\n",
    "    one_hot = pd.get_dummies(df['property_type'], columns=property_columns)\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)\n",
    "    \n",
    "    df['tenure'] = df['tenure'].fillna(value=df.property_type)\n",
    "    df.loc[df.tenure.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('condo', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('terraced house', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('corner', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('landed', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('protected', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('110-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('103-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('102-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('100-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('999-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('946-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('956-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('947-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('929-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df['encoded_tenure'] = 0\n",
    "    df.loc[df.tenure.str.contains('freehold', flags=re.IGNORECASE, regex=True), 'encoded_tenure'] = 1\n",
    "    \n",
    "    df['encoded_furnishing'] = 0\n",
    "    df.loc[df.furnishing.str.contains('partial', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 0.5\n",
    "    df.loc[df.furnishing.str.contains('unfurnished', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = -1\n",
    "    df.loc[df.furnishing.str.contains('fully', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 1\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def pre_process(df, mode='train') -> pd.DataFrame:\n",
    "    df = handle_missing_values(df)\n",
    "    if mode=='train': df = handle_invalid_values(df)\n",
    "    df = transform_data(df)\n",
    "    df = ignore_attributes(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc24d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20060, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_beds</th>\n",
       "      <th>num_baths</th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>price</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>bungalow</th>\n",
       "      <th>condo</th>\n",
       "      <th>corner</th>\n",
       "      <th>hdb</th>\n",
       "      <th>landed</th>\n",
       "      <th>protected</th>\n",
       "      <th>terraced house</th>\n",
       "      <th>walk-up</th>\n",
       "      <th>encoded_tenure</th>\n",
       "      <th>encoded_furnishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1115</td>\n",
       "      <td>514500.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1575</td>\n",
       "      <td>995400.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>8485000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>958</td>\n",
       "      <td>2626000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732</td>\n",
       "      <td>1764000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_beds  num_baths  size_sqft      price  depreciation  bungalow  condo  \\\n",
       "0       3.0        2.0       1115   514500.0          34.0         0      0   \n",
       "1       4.0        2.0       1575   995400.0          30.0         0      0   \n",
       "2       4.0        6.0       3070  8485000.0           0.0         0      1   \n",
       "3       3.0        2.0        958  2626000.0           0.0         0      1   \n",
       "4       2.0        1.0        732  1764000.0           0.0         0      1   \n",
       "\n",
       "   corner  hdb  landed  protected  terraced house  walk-up  encoded_tenure  \\\n",
       "0       0    1       0          0               0        0               0   \n",
       "1       0    1       0          0               0        0               0   \n",
       "2       0    0       0          0               0        0               1   \n",
       "3       0    0       0          0               0        0               1   \n",
       "4       0    0       0          0               0        0               0   \n",
       "\n",
       "   encoded_furnishing  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.5  \n",
       "3                 0.5  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = pre_process(df, mode='train')\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57dcdfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20060, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_beds</th>\n",
       "      <th>num_baths</th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>price</th>\n",
       "      <th>subzone_pop</th>\n",
       "      <th>subzone_pop_density</th>\n",
       "      <th>dist_2_nearest_cc</th>\n",
       "      <th>nearest_cc_count</th>\n",
       "      <th>dist_2_nearest_mrt</th>\n",
       "      <th>nearest_mrt_count</th>\n",
       "      <th>...</th>\n",
       "      <th>bungalow</th>\n",
       "      <th>condo</th>\n",
       "      <th>corner</th>\n",
       "      <th>hdb</th>\n",
       "      <th>landed</th>\n",
       "      <th>protected</th>\n",
       "      <th>terraced house</th>\n",
       "      <th>walk-up</th>\n",
       "      <th>encoded_tenure</th>\n",
       "      <th>encoded_furnishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1115</td>\n",
       "      <td>514500.0</td>\n",
       "      <td>42240.0</td>\n",
       "      <td>42240.0</td>\n",
       "      <td>3.339</td>\n",
       "      <td>0</td>\n",
       "      <td>0.574</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1575</td>\n",
       "      <td>995400.0</td>\n",
       "      <td>15940.0</td>\n",
       "      <td>15940.0</td>\n",
       "      <td>2.402</td>\n",
       "      <td>0</td>\n",
       "      <td>1.734</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>8485000.0</td>\n",
       "      <td>9980.0</td>\n",
       "      <td>9980.0</td>\n",
       "      <td>2.171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>958</td>\n",
       "      <td>2626000.0</td>\n",
       "      <td>6180.0</td>\n",
       "      <td>6180.0</td>\n",
       "      <td>1.606</td>\n",
       "      <td>0</td>\n",
       "      <td>0.726</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732</td>\n",
       "      <td>1764000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.870</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_beds  num_baths  size_sqft      price  subzone_pop  \\\n",
       "0       3.0        2.0       1115   514500.0      42240.0   \n",
       "1       4.0        2.0       1575   995400.0      15940.0   \n",
       "2       4.0        6.0       3070  8485000.0       9980.0   \n",
       "3       3.0        2.0        958  2626000.0       6180.0   \n",
       "4       2.0        1.0        732  1764000.0         80.0   \n",
       "\n",
       "   subzone_pop_density  dist_2_nearest_cc  nearest_cc_count  \\\n",
       "0              42240.0              3.339                 0   \n",
       "1              15940.0              2.402                 0   \n",
       "2               9980.0              2.171                 0   \n",
       "3               6180.0              1.606                 0   \n",
       "4                 80.0              1.870                 0   \n",
       "\n",
       "   dist_2_nearest_mrt  nearest_mrt_count  ...  bungalow  condo  corner  hdb  \\\n",
       "0               0.574                  1  ...         0      0       0    1   \n",
       "1               1.734                  0  ...         0      0       0    1   \n",
       "2               1.320                  0  ...         0      1       0    0   \n",
       "3               0.726                  2  ...         0      1       0    0   \n",
       "4               0.371                  3  ...         0      1       0    0   \n",
       "\n",
       "   landed  protected  terraced house  walk-up  encoded_tenure  \\\n",
       "0       0          0               0        0               0   \n",
       "1       0          0               0        0               0   \n",
       "2       0          0               0        0               1   \n",
       "3       0          0               0        0               1   \n",
       "4       0          0               0        0               0   \n",
       "\n",
       "   encoded_furnishing  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.5  \n",
       "3                 0.5  \n",
       "4                 0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_ = pre_process(df_, mode='train')\n",
    "df_preprocessed_['subzone_pop'].fillna((df_preprocessed_['subzone_pop'].mean()), inplace=True)\n",
    "df_preprocessed_['subzone_pop_density'].fillna((df_preprocessed_['subzone_pop_density'].mean()), inplace=True)\n",
    "print(df_preprocessed_.shape)\n",
    "df_preprocessed_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16048, 14)\n",
      "(16048,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df_preprocessed.loc[:, df_preprocessed.columns != 'price']\n",
    "y_housing = df_preprocessed['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e76f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16048, 26)\n",
      "(16048,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing_ = df_preprocessed_.loc[:, df_preprocessed_.columns != 'price']\n",
    "y_housing_ = df_preprocessed_['price']\n",
    "\n",
    "X_train_, X_val_, y_train_, y_val_ = train_test_split(X_housing_, y_housing_, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "print(X_train_.shape)\n",
    "print(y_train_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87847bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize/Normalize\n",
    "\n",
    "def post_process(df) -> pd.DataFrame:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62567c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = post_process(X_train)\n",
    "X_val = post_process(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a45c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = post_process(X_train_)\n",
    "X_val_ = post_process(X_val_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "(Add outline of steps here later)\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Extra Trees Regressor\n",
    "- Bagging Regressor\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_feature) -> pd.DataFrame: # Fix it. Numpy array is returned\n",
    "    y_hat = model.predict(X_feature)\n",
    "    return y_hat\n",
    "\n",
    "def validate(y, y_hat) -> None:\n",
    "    rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "    print('Validation RMSE: {:.3}'.format(rmse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a76c34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e48c45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split_indices_ = [-1 if x in X_train_.index else 0 for x in X_housing_.index]\n",
    "ps_ = PredefinedSplit(test_fold=val_split_indices_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(X_feature, y_label):\n",
    "    lr = LinearRegression().fit(X_feature, y_label)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e55a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 3.66e+06\n"
     ]
    }
   ],
   "source": [
    "model_lr = train_lr(X_train, y_train)\n",
    "y_hat_lr = predict(model_lr, X_val)\n",
    "validate(y_val, y_hat_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8855595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 3.57e+06\n"
     ]
    }
   ],
   "source": [
    "model_lr_ = train_lr(X_train_, y_train_)\n",
    "y_hat_lr_ = predict(model_lr_, X_val_)\n",
    "validate(y_val_, y_hat_lr_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(X_feature, y_label):\n",
    "    rf = RandomForestRegressor(n_estimators=RF_NUM_ESTIMATORS, criterion=RF_CRITERION, max_depth=RF_MAX_DEPTH,\n",
    "                          min_samples_split=RF_MIN_SPLIT, min_samples_leaf=RF_MIN_LEAF, max_features=RF_MAX_FEATURES,\n",
    "                          bootstrap=RF_BOOTSTRAP).fit(X_feature, y_label)\n",
    "    return rf\n",
    "\n",
    "def bestfit_rf(X_feature, y_label, train_test_split):\n",
    "    estimator = RandomForestRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\"],\n",
    "              'max_features': [1],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_rf = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_rf.fit(X_feature, y_label)\n",
    "    print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "    return model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ad2ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': False, 'criterion': 'squared_error', 'max_depth': 50, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 2.49e+05\n"
     ]
    }
   ],
   "source": [
    "model_rf = bestfit_rf(X_housing, y_housing, ps)\n",
    "y_hat_rf = predict(model_rf, X_val)\n",
    "validate(y_val, y_hat_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef998421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': False, 'criterion': 'squared_error', 'max_depth': 50, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 2.14e+05\n"
     ]
    }
   ],
   "source": [
    "model_rf_ = bestfit_rf(X_housing_, y_housing_, ps_)\n",
    "y_hat_rf_ = predict(model_rf_, X_val_)\n",
    "validate(y_val_, y_hat_rf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb(X_feature, y_label):\n",
    "    gb = GradientBoostingRegressor(n_estimators=GB_NUM_ESTIMATORS, learning_rate=GB_LEARNING_RATE,\n",
    "                                  max_depth=GB_MAX_DEPTH, criterion=GB_CRITERION).fit(X_feature, y_label)\n",
    "    return gb\n",
    "\n",
    "def bestfit_gb(X_feature, y_label, train_test_split):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_gb = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_gb.fit(X_feature, y_label)\n",
    "    print('Gradient Boost Best Parameters: {}'.format(model_gb.best_params_))\n",
    "    return model_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54184b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Best Parameters: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 2.49e+05\n"
     ]
    }
   ],
   "source": [
    "model_gb = bestfit_gb(X_housing, y_housing, ps)\n",
    "y_hat_gb = predict(model_gb, X_val)\n",
    "validate(y_val, y_hat_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "813b833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Best Parameters: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 10, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 4.2e+05\n"
     ]
    }
   ],
   "source": [
    "model_gb_ = bestfit_gb(X_housing_, y_housing_, ps_)\n",
    "y_hat_gb_ = predict(model_gb_, X_val_)\n",
    "validate(y_val_, y_hat_gb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7cf3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_ab():\n",
    "    pass\n",
    "\n",
    "def bestfit_ab(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "    params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "              'base_estimator__splitter': ['best', 'random'],\n",
    "              'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_ab = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_ab.fit(X_feature, y_label)\n",
    "    print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "    return model_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "384e458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 50, 'base_estimator__splitter': 'random', 'learning_rate': 0.01, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 2.75e+05\n"
     ]
    }
   ],
   "source": [
    "model_ab = bestfit_ab(X_housing, y_housing, ps)\n",
    "y_hat_ab = predict(model_ab, X_val)\n",
    "validate(y_val, y_hat_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbf78db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 25, 'base_estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 3.91e+05\n"
     ]
    }
   ],
   "source": [
    "model_ab_ = bestfit_ab(X_housing_, y_housing_, ps_)\n",
    "y_hat_ab_ = predict(model_ab_, X_val_)\n",
    "validate(y_val_, y_hat_ab_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb156ed3",
   "metadata": {},
   "source": [
    "# Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "007bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_et():\n",
    "    pass\n",
    "\n",
    "def bestfit_et(X_feature, y_label, train_test_split):\n",
    "    estimator = ExtraTreesRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_et = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_et.fit(X_feature, y_label)\n",
    "    print('Extra Trees Best Parameters: {}'.format(model_et.best_params_))\n",
    "    return model_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3354aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Best Parameters: {'criterion': 'squared_error', 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 4.71e+05\n"
     ]
    }
   ],
   "source": [
    "model_et = bestfit_et(X_housing, y_housing, ps)\n",
    "y_hat_et = predict(model_et, X_val)\n",
    "validate(y_val, y_hat_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bd18067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Best Parameters: {'criterion': 'friedman_mse', 'max_depth': 50, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 2.14e+05\n"
     ]
    }
   ],
   "source": [
    "model_et_ = bestfit_et(X_housing_, y_housing_, ps_)\n",
    "y_hat_et_ = predict(model_et_, X_val_)\n",
    "validate(y_val_, y_hat_et_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9e7c3",
   "metadata": {},
   "source": [
    "# Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f889699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_br():\n",
    "    pass\n",
    "\n",
    "def bestfit_br(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = BaggingRegressor(base_estimator=base_estimator)\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_br = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_br.fit(X_feature, y_label)\n",
    "    print('Bagging Best Parameters: {}'.format(model_br.best_params_))\n",
    "    return model_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aaaf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Best Parameters: {'max_features': 1, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 4.58e+06\n"
     ]
    }
   ],
   "source": [
    "model_br = bestfit_br(X_housing, y_housing, ps)\n",
    "y_hat_br = predict(model_br, X_val)\n",
    "validate(y_val, y_hat_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4f77681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Best Parameters: {'max_features': 1, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 4.4e+06\n"
     ]
    }
   ],
   "source": [
    "model_br_ = bestfit_br(X_housing_, y_housing_, ps_)\n",
    "y_hat_br_ = predict(model_br_, X_val_)\n",
    "validate(y_val_, y_hat_br_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model here for final predictions - Linear Regression (TODO: CHANGE THIS) \n",
    "# Train over entire training set\n",
    "df_train = pd.read_csv('data/train.csv') \n",
    "df_train = pre_process(df_train, mode='train')\n",
    "X_train = df_train.loc[:, df_train.columns != 'price']\n",
    "y_train = df_train['price']\n",
    "X_train = post_process(X_train)\n",
    "model = train_rf(X_train, y_train)\n",
    "\n",
    "# Predict labels for test set\n",
    "df_test = pd.read_csv('data/test.csv') \n",
    "df_test = pre_process(df_test, mode='test')\n",
    "df_test = post_process(df_test)\n",
    "predictions = predict(model, df_test)\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.to_csv('predictions.csv', header=['Predicted'], index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model here for final predictions - Linear Regression (TODO: CHANGE THIS) \n",
    "# Train over entire training set\n",
    "df_train_ = pd.read_csv('data/train.csv')\n",
    "df_train_ = improve_dataset(df_train_)\n",
    "df_train_ = pre_process(df_train_, mode='train')\n",
    "df_train_['subzone_pop'].fillna((df_train_['subzone_pop'].mean()), inplace=True)\n",
    "df_train_['subzone_pop_density'].fillna((df_train_['subzone_pop_density'].mean()), inplace=True)\n",
    "X_train_ = df_train_.loc[:, df_train_.columns != 'price']\n",
    "y_train_ = df_train_['price']\n",
    "X_train_ = post_process(X_train_)\n",
    "#model_ = train_lr(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9667986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for test set\n",
    "df_test_ = pd.read_csv('data/test.csv') \n",
    "df_test_ = improve_dataset(df_test_)\n",
    "df_test_ = pre_process(df_test_, mode='test')\n",
    "df_test_['subzone_pop'].fillna((df_test_['subzone_pop'].mean()), inplace=True)\n",
    "df_test_['subzone_pop_density'].fillna((df_test_['subzone_pop_density'].mean()), inplace=True)\n",
    "df_test = post_process(df_test_)\n",
    "#predictions = predict(model_, df_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "484e9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = train_rf(X_train_, y_train_)\n",
    "predictions = predict(model_, df_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "619bb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.to_csv(\"predictions.csv\", header=[\"Predicted\"], index=True, index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65844b9",
   "metadata": {},
   "source": [
    "# Do not execute\n",
    "\n",
    "(Add misc. code here that was not utilized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37143e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with property details\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc176cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.99.co/singapore/condos-apartments/meyerhouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.status_code)\n",
    "print(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The website has anti-crawl software on it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
