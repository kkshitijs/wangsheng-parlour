{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c567016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanata/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility of notebook\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20254, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "(Generate plots here; preprocessing steps follow below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2a9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trivial modifications here(DO NOT NORMALIZE/IMPUTE YET)\n",
    "\n",
    "# Drop listing id; nominal identifier with no meaning\n",
    "df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "# Drop elevation; all the values are 0, spurious attribute\n",
    "df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "# Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "# Drop invalid data with negative or zero-valued price\n",
    "df = df[df.price > 0]\n",
    "\n",
    "# BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "df.drop('title', axis=1, inplace=True)\n",
    "df.drop('address', axis=1, inplace=True)\n",
    "df.drop('property_name', axis=1, inplace=True)\n",
    "df.drop('property_type', axis=1, inplace=True)\n",
    "df.drop('tenure', axis=1, inplace=True)\n",
    "df.drop('built_year', axis=1, inplace=True)\n",
    "df.drop('num_beds', axis=1, inplace=True)\n",
    "df.drop('num_baths', axis=1, inplace=True)\n",
    "df.drop('floor_level', axis=1, inplace=True)\n",
    "df.drop('furnishing', axis=1, inplace=True)\n",
    "df.drop('available_unit_types', axis=1, inplace=True)\n",
    "df.drop('total_num_units', axis=1, inplace=True)\n",
    "df.drop('lat', axis=1, inplace=True)\n",
    "df.drop('lng', axis=1, inplace=True)\n",
    "df.drop('subzone', axis=1, inplace=True)\n",
    "df.drop('planning_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc24d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20153, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b8afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df.loc[:, df.columns != 'price']\n",
    "y_housing = df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd370a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform further pre-processing steps only on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "(Add outline of steps here later)\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Validation rmse: 5.85e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "rmse_linreg = mean_squared_error(y_val, y_hat_linreg, squared=False)\n",
    "\n",
    "print('Linear Regression Validation rmse: {:.3}'.format(rmse_linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': True, 'criterion': 'squared_error', 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Random Forest Validation rmse: 2.98e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\"],\n",
    "          'max_features': [1],\n",
    "          'bootstrap': [True, False],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_rf = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_rf.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_rf = model_rf.predict(X_val)\n",
    "rmse_rf = mean_squared_error(y_val, y_hat_rf, squared=False)\n",
    "\n",
    "print('Random Forest Validation rmse: {:.3}'.format(rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 5, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Gradient Boosting Validation rmse: 3.51e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "estimator = GradientBoostingRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "          'max_features': [1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_gb = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_gb.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_gb.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_gb = model_gb.predict(X_val)\n",
    "rmse_gb = mean_squared_error(y_val, y_hat_gb, squared=False)\n",
    "\n",
    "print('Gradient Boosting Validation rmse: {:.3}'.format(rmse_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 50, 'base_estimator__splitter': 'best', 'learning_rate': 1, 'n_estimators': 25, 'random_state': 42}\n",
      "AdaBoost Validation rmse: 2.97e+06\n"
     ]
    }
   ],
   "source": [
    "base_estimator = DecisionTreeRegressor()\n",
    "estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "          'base_estimator__splitter': ['best', 'random'],\n",
    "          'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_ab = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_ab.fit(X_housing, y_housing)\n",
    "\n",
    "print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_ab = model_ab.predict(X_val)\n",
    "rmse_ab = mean_squared_error(y_val, y_hat_ab, squared=False)\n",
    "\n",
    "print('AdaBoost Validation rmse: {:.3}'.format(rmse_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanata/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 Batches processed | 99] Loss: 74088216.0000\n",
      "Epoch [1 Batches processed | 199] Loss: 3812634.0000\n",
      "Epoch [1 Batches processed | 299] Loss: 4383063.5000\n",
      "Epoch [1 Batches processed | 399] Loss: 13035016.0000\n",
      "Epoch [1 Batches processed | 499] Loss: 3967107.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanata/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2 Batches processed | 99] Loss: 4189813.0000\n",
      "Epoch [2 Batches processed | 199] Loss: 3987484.7500\n",
      "Epoch [2 Batches processed | 299] Loss: 4095743.7500\n",
      "Epoch [2 Batches processed | 399] Loss: 82362232.0000\n",
      "Epoch [2 Batches processed | 499] Loss: 4018017.5000\n",
      "Epoch [3 Batches processed | 99] Loss: 3835343.0000\n",
      "Epoch [3 Batches processed | 199] Loss: 82014840.0000\n",
      "Epoch [3 Batches processed | 299] Loss: 3885355.5000\n",
      "Epoch [3 Batches processed | 399] Loss: 4321278.5000\n",
      "Epoch [3 Batches processed | 499] Loss: 4217107.0000\n",
      "Epoch [4 Batches processed | 99] Loss: 4044569.2500\n",
      "Epoch [4 Batches processed | 199] Loss: 73373376.0000\n",
      "Epoch [4 Batches processed | 299] Loss: 4110977.2500\n",
      "Epoch [4 Batches processed | 399] Loss: 4052472.2500\n",
      "Epoch [4 Batches processed | 499] Loss: 13030081.0000\n",
      "Epoch [5 Batches processed | 99] Loss: 4105910.7500\n",
      "Epoch [5 Batches processed | 199] Loss: 73418200.0000\n",
      "Epoch [5 Batches processed | 299] Loss: 3910133.7500\n",
      "Epoch [5 Batches processed | 399] Loss: 4246490.0000\n",
      "Epoch [5 Batches processed | 499] Loss: 13013023.0000\n",
      "Epoch [6 Batches processed | 99] Loss: 82437800.0000\n",
      "Epoch [6 Batches processed | 199] Loss: 4220987.0000\n",
      "Epoch [6 Batches processed | 299] Loss: 3851550.0000\n",
      "Epoch [6 Batches processed | 399] Loss: 4288211.0000\n",
      "Epoch [6 Batches processed | 499] Loss: 3989931.0000\n",
      "Epoch [7 Batches processed | 99] Loss: 3861683.0000\n",
      "Epoch [7 Batches processed | 199] Loss: 4397511.5000\n",
      "Epoch [7 Batches processed | 299] Loss: 3927601.2500\n",
      "Epoch [7 Batches processed | 399] Loss: 12824812.0000\n",
      "Epoch [7 Batches processed | 499] Loss: 73668688.0000\n",
      "Epoch [8 Batches processed | 99] Loss: 4424376.0000\n",
      "Epoch [8 Batches processed | 199] Loss: 4382395.5000\n",
      "Epoch [8 Batches processed | 299] Loss: 3866507.5000\n",
      "Epoch [8 Batches processed | 399] Loss: 73210584.0000\n",
      "Epoch [8 Batches processed | 499] Loss: 12925530.0000\n",
      "Epoch [9 Batches processed | 99] Loss: 4385586.0000\n",
      "Epoch [9 Batches processed | 199] Loss: 3906684.2500\n",
      "Epoch [9 Batches processed | 299] Loss: 73523296.0000\n",
      "Epoch [9 Batches processed | 399] Loss: 12865217.0000\n",
      "Epoch [9 Batches processed | 499] Loss: 4206032.5000\n",
      "Epoch [10 Batches processed | 99] Loss: 12781725.0000\n",
      "Epoch [10 Batches processed | 199] Loss: 73639928.0000\n",
      "Epoch [10 Batches processed | 299] Loss: 4634209.5000\n",
      "Epoch [10 Batches processed | 399] Loss: 3830202.2500\n",
      "Epoch [10 Batches processed | 499] Loss: 3886769.0000\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "**Wrap-up**\n",
    "\n",
    "(Do this only before submitting to Kaggle; train over the entire set here after the hyperparameters are identified; then perform testing and submit results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv') \n",
    "# Pre-process similar to above. Need to refactor into a function.\n",
    "\n",
    "# Linear Regression\n",
    "linreg = LinearRegression().fit(X_housing, y_housing)\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65844b9",
   "metadata": {},
   "source": [
    "**Do not execute**\n",
    "\n",
    "(Add misc. code here that was not utilized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
