{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba5b0",
   "metadata": {},
   "source": [
    "# Task 1: Housing Price Regression\n",
    "\n",
    "This task is focused on predicting the price of a house listing based on a pre-determined set of features. This notebook is structured as follows:\n",
    "\n",
    "1. Setup\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Model Evaluation\n",
    "4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a287a",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "The following section is required to setup this notebook. Import the appropriate modules and modify model hyperparameters here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re\n",
    "from wangsheng_utils import *\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility\n",
    "BEST_MODEL = 'rf' # Choose best regression model based on evaluation\n",
    "TRAIN_SIZE = 0.8 # Fraction of training set used for model evaluation; the remaining split is validation set\n",
    "RADIUS_OF_INFLUENCE_KM = 0.5 # Radius of interest for a location centered on their geographical coordinates\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 50\n",
    "RF_MAX_DEPTH = 40\n",
    "RF_MAX_FEATURES = 2\n",
    "RF_MIN_SPLIT = 3\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = False\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 50\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "GB_LEARNING_RATE = 0.01\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 50\n",
    "AB_MAX_DEPTH = 30\n",
    "AB_SPLITTER = \"best\"\n",
    "AB_LEARNING_RATE = 0.01\n",
    "\n",
    "# Extra Trees Regressor: Fill in based on GridSearch results\n",
    "ET_NUM_ESTIMATORS = 50\n",
    "ET_MAX_DEPTH = 40\n",
    "ET_MAX_FEATURES = 2\n",
    "ET_MIN_SPLIT = 3\n",
    "ET_MIN_LEAF = 1\n",
    "ET_BOOTSTRAP = False\n",
    "ET_CRITERION= \"squared_error\"\n",
    "\n",
    "# Bagging Regressor: Fill in based on GridSearch results\n",
    "BR_NUM_ESTIMATORS = 100\n",
    "BR_MAX_DEPTH = 50\n",
    "BR_SPLITTER = \"best\"\n",
    "BR_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training methods\n",
    "def train(X_feature, y_label, model_name='rf'):\n",
    "    model = None\n",
    "    if model_name=='lr':\n",
    "        model = LinearRegression().fit(X_feature, y_label)\n",
    "    elif model_name=='rf':\n",
    "        model = RandomForestRegressor(n_estimators=RF_NUM_ESTIMATORS, criterion=RF_CRITERION, max_depth=RF_MAX_DEPTH,\n",
    "                      min_samples_split=RF_MIN_SPLIT, min_samples_leaf=RF_MIN_LEAF, max_features=RF_MAX_FEATURES,\n",
    "                      bootstrap=RF_BOOTSTRAP, random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    elif model_name=='gb':\n",
    "        model = GradientBoostingRegressor(n_estimators=GB_NUM_ESTIMATORS, learning_rate=GB_LEARNING_RATE,\n",
    "                               max_depth=GB_MAX_DEPTH, criterion=GB_CRITERION, \n",
    "                               random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    elif model_name=='ab':\n",
    "        dt = DecisionTreeRegressor(max_depth=AB_MAX_DEPTH, splitter=AB_SPLITTER,\n",
    "                                   random_state=RANDOM_CONTROL)\n",
    "        model = AdaBoostRegressor(base_estimator=dt, n_estimators=AB_NUM_ESTIMATORS, learning_rate=AB_LEARNING_RATE,\n",
    "                              random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    elif model_name=='et':\n",
    "        model = ExtraTreesRegressor(n_estimators=ET_NUM_ESTIMATORS, criterion=ET_CRITERION, max_depth=ET_MAX_DEPTH,\n",
    "                              min_samples_split=ET_MIN_SPLIT, min_samples_leaf=ET_MIN_LEAF, max_features=ET_MAX_FEATURES,\n",
    "                              bootstrap=ET_BOOTSTRAP, random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    elif model_name=='br':\n",
    "        dt = DecisionTreeRegressor(max_depth=BG_MAX_DEPTH, splitter=BG_SPLITTER,\n",
    "                                   random_state=RANDOM_CONTROL)\n",
    "        model = BaggingRegressor(base_estimator=dt, n_estimators=BR_NUM_ESTIMATORS, learning_rate=BR_LEARNING_RATE,\n",
    "                             random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    elif model_name=='nn':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model\n",
    "            \n",
    "# Define model prediction method\n",
    "def predict(model, X_feature) -> pd.DataFrame: # Fix it. Numpy array is returned\n",
    "    y_hat = model.predict(X_feature)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We first explore our given training data. This is done via the following sub-tasks:\n",
    "\n",
    "- Visualize\n",
    "- Pre-process\n",
    "- Post-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75e072",
   "metadata": {},
   "source": [
    "## 2a. Visualize\n",
    "\n",
    "Here, we take our first look at the data to get an idea of how the attribute values are spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e14dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.tick_params(labelsize=10)\n",
    "    df['price_per_square_ft'] = df['price']/df['size_sqft']\n",
    "    \n",
    "    filter_price_sqft_hdb = ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) & (df.title.str.contains('hdb','Hdb', regex=False)))\n",
    "    df[filter_price_sqft_hdb][['property_type','num_baths','num_beds','size_sqft','price','price_per_square_ft']]\n",
    "    plot1 = df[filter_price_sqft_hdb][['property_type','num_baths','num_beds','size_sqft','price','price_per_square_ft']]\n",
    "    \n",
    "    plt.title(\"HDB price per square feet($/ft2) vs square feet(ft2)\")\n",
    "    plt.scatter(plot1['price_per_square_ft'],plot1['size_sqft'])\n",
    "    plt.xlabel('price per square ft ($/ft2)', fontsize=16)\n",
    "    plt.ylabel('size_sqft (ft2)', fontsize=16)\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    filter_1 = ((df['price_per_square_ft'] < 10000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) & (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plot2 = df[filter_1][['property_type','num_baths','num_beds','size_sqft','price','price_per_square_ft']]\n",
    "    plt.title(\"HDB price per square feet($/ft2) vs square feet(ft2)\")\n",
    "    plt.scatter(plot2['price_per_square_ft'],plot2['size_sqft'])\n",
    "    plt.xlabel('price per square ft ($/ft2)', fontsize=16)\n",
    "    plt.ylabel('size_sqft (ft2)', fontsize=16)\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    filter_2 = ((df['price_per_square_ft'] < 2000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) & (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plot3 = df[filter_2][['property_type','num_baths','num_beds','size_sqft','price','price_per_square_ft']]\n",
    "    plt.title(\"HDB price per square feet($/ft2) vs square feet(ft2)\")\n",
    "    plt.scatter(plot3['price_per_square_ft'],plot3['size_sqft'])\n",
    "    plt.xlabel('price per square ft ($/ft2)', fontsize=16)\n",
    "    plt.ylabel('size_sqft (ft2)', fontsize=16)\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x}\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    filter_3 = ((df['price_per_square_ft'] > 1400) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) & (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    \n",
    "    table1 = df[filter_3][['title','property_type','num_baths','num_beds','size_sqft','price','price_per_square_ft']].sort_values(by=['price_per_square_ft'])\n",
    "    pd.options.display.float_format = '{:.0f}'.format\n",
    "    display(table1)\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100bab",
   "metadata": {},
   "source": [
    "## 2b. Pre-process\n",
    "\n",
    "Here, we go through the training data and use the information gained in the section above to pick and choose what attributes we wish to consider in our analysis as well as their representation. \n",
    "\n",
    "We call this _pre-processing_ as we are working on the whole training set here. To avoid information leakage, we ensure that we do not look at the label data unless required to check for invalid inputs(more on this below). Additionally, we do not perform any aggregation or imputation here to ensure that the validation set does not influence our training set for the following sections.\n",
    "\n",
    "The _pre-processing_ is comprised of the following tasks which are performed in order:\n",
    "\n",
    "- Remove invalid values\n",
    "- Remove outliers\n",
    "- Handle missing values\n",
    "- Data Transform\n",
    "- Data Augmentation\n",
    "- Ignore attributes\n",
    "\n",
    "TODO: Talk about each attribute here. \n",
    "\n",
    "For HDB,\n",
    "\n",
    "Assume that num_beds refers only to the bedrooms excluding the living rooms\n",
    "* Hdb 2-room = 1 bedroom , 1 bathroom\n",
    "* Hdb 3-room = 2 bedroom , 2 bathroom\n",
    "* Hdb 4-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 5-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 3-gen = 4 bedroom , 3 bathroom\n",
    "* Hdb Executive = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Masionette = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Jumbo = 4 bedroom, 4 bathroom\n",
    "\n",
    "References:\n",
    "* http://www.data.com.sg/template-m.jsp?p=my/1.html \n",
    "* https://www.hdb.gov.sg/residential/buying-a-flat/finding-a-flat/types-of-flats\n",
    "\n",
    "OneMap:\n",
    "\n",
    "* https://www.onemap.gov.sg/docs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_values(df) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Price is the target regression variable. If negative or 0, treat that row as invalid (logical assumption)\n",
    "    if 'price' in df:\n",
    "        df = df[df.price > 0]\n",
    "\n",
    "    # Filter out HDB prices more than 2,000,000 (real-world knowledge; ref. source)\n",
    "    if 'price' in df:\n",
    "        filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "        df_dropped = df[filter_price_hdb]\n",
    "        df = df.drop(df[filter_price_hdb].index)\n",
    "        print(\"Filterout HDB prices more than 2,000,000\")\n",
    "        display(df_dropped)\n",
    "        \n",
    "    # Filter out HDB with more bathrooms than bedrooms (real-world knowledge; ref. source)\n",
    "    filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df_dropped = df[filter_bath_beds_hdb]\n",
    "    df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "    print(\"Filter out HDB with more bathrooms than bedrooms\")\n",
    "    display(df_dropped)\n",
    "\n",
    "    # Filter out HDB with more than 4 bathrooms or 6 bedrooms (real-world knowledge; ref. source)\n",
    "    filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 6)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df_dropped = df[filter_bath_beds_4_hdb]\n",
    "    df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "    print(\"Filter out HDB with more than 4 bathrooms or 6 bedrooms\")\n",
    "    display(df_dropped)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Filter out HDB with size > 2500 sqft (outlier detection)\n",
    "    filter_size_hdb = ((df.size_sqft > 2500) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df[filter_size_hdb][['property_type','num_baths','num_beds','size_sqft','price']]\n",
    "    df_dropped = df[filter_size_hdb]\n",
    "    df = df.drop(df[filter_size_hdb].index)\n",
    "    print(\"Filter out HDB with size > 2500 sqft (outlier detection)\")\n",
    "    display(df_dropped)\n",
    "    \n",
    "    # Target label is used below for outlier detection. Appropriately tagged data is dropped entirely.\n",
    "    if 'price' in df:\n",
    "    # Filtering those data with less than $200/square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_price_sqft_200 = ((df['price per sq ft'] < 200) & (df['price per sq ft'] > 0))\n",
    "        df_dropped = df[filter_price_sqft_200]\n",
    "        df = df.drop(df[filter_price_sqft_200].index)\n",
    "        print(\"Filtering those data with less than $200/square feet\")\n",
    "        display(df_dropped)\n",
    "    # Filtering those data with less than 500 square feet, and more than $5000 per square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_size = ((df['price per sq ft'] > 5000) & (df['price per sq ft'] > 0) & (df['size_sqft'] < 500))\n",
    "        df_dropped = df[filter_size]\n",
    "        df = df.drop(df[filter_size].index)\n",
    "        print(\"Filtering those data with less than 500 square feet, and more than $5000 per square feet\")\n",
    "        display(df_dropped)\n",
    "    # Filter out HDB more than $1600 per square feet and since last sold highest price for hdb is $1400\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_price_hdb_2 = ( (df['price per sq ft']>1600) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "        \n",
    "        df_dropped = df[filter_price_hdb_2]\n",
    "        df = df.drop(df[filter_price_hdb_2].index)\n",
    "        display(df_dropped)\n",
    "           \n",
    "        df.drop('price per sq ft', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, mode='train') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Treat missing year data as new.\n",
    "    # Semantically, we define this attribute as the depreciation factor for pricing.\n",
    "    # A new house or one with missing data denotes the depreciation factor is 0 or unknown.\n",
    "    # The depreciation factor is assumed to be the difference between construction and current year.\n",
    "    # TODO: Maybe do not treat future years as current! Inflation factor might be one to look out for.\n",
    "    df['built_year'] = df['built_year'].fillna(2022)\n",
    "    \n",
    "    # TODO: 80 are missing. Should we remove them or should we keep it as 0?\n",
    "    # Verify assumption if studio qualifies as 1 bed. \n",
    "    # 75 of missing are studio, we replace the Nan as 1\n",
    "    filter_beds_studio = ((df.num_beds.isna()) & ((df.title.str.contains('studio','Studio', flags=re.IGNORECASE, regex=True))))\n",
    "    df.loc[filter_beds_studio, \"num_beds\"] = 1\n",
    "    \n",
    "    ## HDB\n",
    "    ## bathrooms\n",
    "    ## Fill missing data for HDB, bathrooms\n",
    "         \n",
    "    ## Fill bathroom = 1 For HDB size 2-room\n",
    "    df.loc[(df['num_beds'] <= 2) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 1\n",
    "    ## Fill bathroom = 2 For HDB size 3-room, 4-room, 5-room\n",
    "    df.loc[(df['num_beds'] < 4) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bathroom = 3 For HDB Executive and Mansionette\n",
    "    df.loc[(df['num_beds'] == 4) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 3\n",
    "    ## Fill bathroom = 4 For HDB jumbo\n",
    "    df.loc[(df['num_beds'] > 4) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 4\n",
    "\n",
    "\n",
    "    ## If number of beds are missing, then we use size as a gauge\n",
    "\n",
    "    ## Fill bathroom = 1 For HDB size 500 sq ft and below\n",
    "    df.loc[(df['size_sqft'] <= 500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 1\n",
    "    ## Fill bathroom = 2 For HDB size 1500 sq ft and below\n",
    "    df.loc[(df['size_sqft'] <= 1500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bathroom = 3 For HDB size between 1500 sq ft and 2000 sq ft\n",
    "    df.loc[(df['size_sqft'] > 1500) & (df['size_sqft'] <= 2000) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 3\n",
    "    ## Fill bathroom = 4 For HDB size between 2000 sq ft and 2500 sq ft\n",
    "    df.loc[(df['size_sqft'] > 2000) & (df['size_sqft'] <= 2500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_baths.isna()), \"num_baths\"] = 4\n",
    "    \n",
    "    \n",
    "    ## HDB\n",
    "    ## bedrooms\n",
    "    ## Fill missing data for HDB, bedrooms\n",
    "\n",
    "    ## Fill bedrooms = 1 For HDB size 500 sq ft and below\n",
    "    df.loc[(df['num_baths'] == 1) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 2\n",
    "    ## Fill bedrooms = 2 For HDB size 3-room, 4-room, 5-room\n",
    "    df.loc[(df['num_baths'] == 2) & (df['size_sqft'] <= 800) &(df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 2\n",
    "    ## Fill bedrooms = 3 For HDB Executive and Mansionette\n",
    "    df.loc[(df['num_baths'] == 2) & (df['size_sqft'] <= 1200) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 3\n",
    "    ## Fill bedrooms = 3 For HDB jumbo\n",
    "    df.loc[(df['num_baths'] >= 2) & (df['size_sqft'] <= 1500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 4 For HDB jumbo\n",
    "    df.loc[(df['num_baths'] > 2) & (df['size_sqft'] <= 2000) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 5 For HDB jumbo\n",
    "    df.loc[(df['num_baths'] > 3) & (df['size_sqft'] <= 2200) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 5\n",
    "    ## Fill bedrooms = 6 For HDB jumbo\n",
    "    df.loc[(df['num_baths'] > 3) & (df['size_sqft'] <= 2500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 6\n",
    "\n",
    "    ## If number of baths are missing, then we use size as a gauge\n",
    "\n",
    "    ## Fill bedrooms = 2 For HDB size 3-room, 4-room, 5-room\n",
    "    df.loc[(df['size_sqft'] <= 800) &(df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 2\n",
    "    ## Fill bedrooms = 3 For HDB Executive and Mansionette\n",
    "    df.loc[(df['size_sqft'] <= 1200) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 3\n",
    "    ## Fill bedrooms = 3 For HDB jumbo\n",
    "    df.loc[(df['size_sqft'] <= 1500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 4 For HDB jumbo\n",
    "    df.loc[(df['size_sqft'] <= 2000) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 5 For HDB jumbo\n",
    "    df.loc[(df['size_sqft'] <= 2200) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 5\n",
    "    ## Fill bedrooms = 6 For HDB jumbo\n",
    "    df.loc[(df['size_sqft'] <= 2500) & (df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) & (df.num_beds.isna()), \"num_beds\"] = 6\n",
    " \n",
    "    ## Condos\n",
    "    ## bathrooms\n",
    "    ## Fill missing data for condos,apartments bathrooms\n",
    "         \n",
    "    ## Fill bathroom = 1 For Condo, apartment size 2-room\n",
    "    df.loc[(df['num_beds'] <= 2) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 1\n",
    "    ## Fill bathroom = 2 For Condo, apartment size 2-room size 3-room, 4-room, 5-room\n",
    "    df.loc[(df['num_beds'] <= 4) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bathroom = 3 For Condo Executive\n",
    "    df.loc[(df['num_beds'] <= 5) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 3\n",
    "    ## Fill bathroom = 4 For Huge Condos\n",
    "    df.loc[(df['num_beds'] > 5) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 4\n",
    "\n",
    "\n",
    "    ## If number of beds are missing, then we use size as a gauge\n",
    "\n",
    "    ## Fill bathroom = 1 For Condos less than 800 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 800) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 1\n",
    "    ## Fill bathroom = 2 For Condos between 800 sq ft and 1300 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1300) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bathroom = 3 For Condos between 1300 sq ft and 1600 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1600) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 3\n",
    "    ## Fill bathroom = 4 For Huge Condos\n",
    "    df.loc[(df['size_sqft'] > 1600) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_baths.isna()), \"num_baths\"] = 4\n",
    "    \n",
    "    ## Condos\n",
    "    ## bedrooms\n",
    "    \n",
    "    ## Fill bedrooms = 2  For Condos between 800 sq ft and 1300 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 800) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_beds.isna()), \"num_beds\"] = 2\n",
    "    ## Fill bedrooms = 3  For Condos between 850 sq ft and 1250 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1250) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_beds.isna()), \"num_beds\"] = 3\n",
    "    ## Fill bedrooms = 4  For Condos between 1250 sq ft and 1500 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1500) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 5  For Condos more than 1500 sq ft\n",
    "    df.loc[(df['size_sqft'] > 1500) & ((df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False))) & (df.num_beds.isna()), \"num_beds\"] = 5\n",
    "    \n",
    "    ## All other houses\n",
    "    ## bedrooms\n",
    "    \n",
    "    ## Fill bedrooms = 2  For houses between 800 sq ft and 1300 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 800) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_beds.isna()), \"num_beds\"] = 2\n",
    "    ## Fill bedrooms = 3  For houses between 850 sq ft and 1250 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1250) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_beds.isna()), \"num_beds\"] = 3\n",
    "    ## Fill bedrooms = 4  For houses between 1250 sq ft and 1500 sq ft\n",
    "    df.loc[(df['size_sqft'] <= 1500) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_beds.isna()), \"num_beds\"] = 4\n",
    "    ## Fill bedrooms = 5  For houses more than 1500 sq ft\n",
    "    df.loc[(df['size_sqft'] > 1500) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_beds.isna()), \"num_beds\"] = 5\n",
    "    \n",
    "    ## All other houses\n",
    "    ## bathrooms\n",
    "    \n",
    "    ## Fill bathrooms = 1  For houses with 1 bed\n",
    "    df.loc[(df['num_beds'] == 1) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_baths.isna()), \"num_baths\"] = 1\n",
    "    ## Fill bathrooms = 2  For houses with 2 beds\n",
    "    df.loc[(df['num_beds'] == 2) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bedrooms = 2  For houses with 3 beds\n",
    "    df.loc[(df['num_beds'] == 3 ) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_baths.isna()), \"num_baths\"] = 2\n",
    "    ## Fill bedrooms = 3  For houses with 4 beds\n",
    "    df.loc[(df['num_beds'] == 4) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_baths.isna()), \"num_baths\"] = 3\n",
    "    ## Fill bedrooms = 4  For houses with 5 or more beds\n",
    "    df.loc[(df['num_beds'] > 4) & (~((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('condo','Condo', flags=re.IGNORECASE, regex=False)) | (df.property_type.str.contains('apart','Apart', flags=re.IGNORECASE, regex=False)))) & (df.num_baths.isna()), \"num_baths\"] = 4\n",
    "    \n",
    "    if mode=='train':\n",
    "        # 5 of missing, we do not have much info. Dropping these 5 data\n",
    "        filter_drop_beds = df.num_beds.isna()\n",
    "        df_dropped = df[filter_drop_beds]\n",
    "        df = df.drop(df[filter_drop_beds].index)\n",
    "        print(\"Dropped missing beds\")\n",
    "        display(df_dropped)\n",
    "\n",
    "        # TODO: 400 are missing. Cannot remove so many data. Use 0 to denote absence of attribute.\n",
    "        filter_drop_baths = df.num_baths.isna()\n",
    "        df = df.drop(df[filter_drop_baths].index)\n",
    "    elif mode=='test':\n",
    "        \n",
    "\n",
    "        # TODO: Filling with 0 for test data. Some imputation might be better.\n",
    "        df['num_beds'] = df['num_beds'].fillna(0)\n",
    "        df['num_baths'] = df['num_baths'].fillna(0)\n",
    "                \n",
    "    # Handle missing coordinates data\n",
    "    for idx in df.index:\n",
    "        if pd.isnull(df.at[idx, 'lat']) or pd.isnull(df.at[idx, 'lng']) or pd.isnull(df.at[idx, 'subzone']):\n",
    "            lat, lng = find_lat_lng(df.at[idx, 'address'])\n",
    "            if lat is not None:\n",
    "                df.at[idx, 'lat'] = float(lat)\n",
    "                #print(\"Fixed address {} with latitude: {}\".format(df.at[idx, 'address'], df.at[idx, 'lat']))\n",
    "            if lng is not None:\n",
    "                df.at[idx, 'lng'] = float(lng)\n",
    "                #print(\"Fixed address {} with longitude: {}\".format(df.at[idx, 'address'], df.at[idx, 'lng']))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # TODO: Test against not doing this.\n",
    "    df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "    \n",
    "    # Convert built_year into the aforementioned depreciation factor\n",
    "    df[\"depreciation\"] = (2022-df[\"built_year\"])\n",
    "    '''\n",
    "    # TODO: Add details on why we are doing so\n",
    "    df.loc[df.property_type.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'property_type'] = 'hdb'\n",
    "    df.loc[df.property_type.str.contains('condo', flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df['property_type'] = df['property_type'].str.lower()\n",
    "    df.loc[df.property_type.str.contains('cluster house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('townhouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('land only', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('apartment',  flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df.loc[df.property_type.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'property_type'] = 'bungalow'\n",
    "    df.loc[df.property_type.str.contains('semi-detached house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('corner terrace',flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('shophouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    df.loc[df.property_type.str.contains('conservation house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    \n",
    "    # Get one hot encoding of columns property_type\n",
    "    one_hot = pd.get_dummies(df['property_type'])\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)\n",
    "    '''\n",
    "    '''\n",
    "    df['tenure'] = df['tenure'].fillna(value=df.property_type)\n",
    "    df.loc[df.tenure.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('condo', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('terraced house', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('corner', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('landed', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('protected', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('110-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('103-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('102-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('100-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('999-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('946-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('956-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('947-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('929-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df['encoded_tenure'] = 0\n",
    "    df.loc[df.tenure.str.contains('freehold', flags=re.IGNORECASE, regex=True), 'encoded_tenure'] = 1\n",
    "    '''\n",
    "    \n",
    "    df['encoded_furnishing'] = 0\n",
    "    df.loc[df.furnishing.str.contains('partial', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 0.5\n",
    "    df.loc[df.furnishing.str.contains('unfurnished', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = -1\n",
    "    df.loc[df.furnishing.str.contains('fully', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1618536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df, distance=1) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # get the files containing the infrastructures data\n",
    "    cc = pd.read_csv(\"data/auxiliary-data/sg-commerical-centres.csv\")\n",
    "    mrt = pd.read_csv(\"data/auxiliary-data/sg-mrt-stations.csv\")\n",
    "    ps = pd.read_csv(\"data/auxiliary-data/sg-primary-schools.csv\")\n",
    "    ss = pd.read_csv(\"data/auxiliary-data/sg-secondary-schools.csv\")\n",
    "    sm = pd.read_csv(\"data/auxiliary-data/sg-shopping-malls.csv\")\n",
    "    sz = pd.read_csv(\"data/auxiliary-data/sg-subzones.csv\")\n",
    "\n",
    "    # calculate distance to nearest commercial center\n",
    "    df['dist_2_nearest_cc'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], cc), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_cc'] = df['dist_2_nearest_cc'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of commercial centers within x km\n",
    "    df['nearest_cc_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], cc, distance), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest mrt station\n",
    "    df['dist_2_nearest_mrt'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], mrt), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_mrt'] = df['dist_2_nearest_mrt'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of mrt stations within x km\n",
    "    df['nearest_mrt_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], mrt, distance), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest primary school\n",
    "    df['dist_2_nearest_ps'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], ps), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_ps'] = df['dist_2_nearest_ps'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of primary schools within x km\n",
    "    df['nearest_ps_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], ps, distance), \n",
    "        axis=1)\n",
    "    # calculate distance to nearest secondary school\n",
    "    df['dist_2_nearest_ss'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], ss), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_ss'] = df['dist_2_nearest_ss'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of secondary schools within x km\n",
    "    df['nearest_ss_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], ss, distance), \n",
    "        axis=1)\n",
    "    \n",
    "    # calculate distance to nearest shopping mall\n",
    "    df['dist_2_nearest_sm'] = df.apply(\n",
    "        lambda row: find_nearest_distance(row['lat'], row['lng'], sm), \n",
    "        axis=1)\n",
    "    df['dist_2_nearest_sm'] = df['dist_2_nearest_sm'].round(decimals=3)\n",
    "\n",
    "    # calculate the number of secondary schools within x km\n",
    "    df['nearest_sm_count'] = df.apply(\n",
    "        lambda row: count_nearest(row['lat'], row['lng'], sm, distance), \n",
    "        axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_attributes(df) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Drop listing id; nominal identifier with no meaning\n",
    "    df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "    # Drop elevation; all the values are 0, spurious attribute\n",
    "    df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "    # Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "    df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "    # Drop floor level as 83% missing and sparse with the rest of the values. \n",
    "    # Not enough data available to get the model trained.\n",
    "    df.drop('floor_level', axis=1, inplace=True)\n",
    "\n",
    "    # Drop column property_type and tenure as it is now encoded\n",
    "    df.drop('property_type',axis = 1, inplace=True)\n",
    "    df.drop('tenure',axis=1, inplace=True)\n",
    "\n",
    "    # Drop latitude and longitude; using it in conjunction with auxiliary data\n",
    "    # Actual values are not semantically useful\n",
    "    df.drop('lat', axis=1, inplace=True)\n",
    "    df.drop('lng', axis=1, inplace=True)\n",
    "    # Drop planning area to avoid dimensional bloat\n",
    "    # Categories replaced with numerical identifiers from auxiliary data\n",
    "    df.drop('planning_area', axis=1, inplace=True)\n",
    "    # Address used in conjunction with augmentation\n",
    "    df.drop('address', axis=1, inplace=True)\n",
    "\n",
    "    # Drop this attribute as a significant amount of data(27.9%) is missing here.\n",
    "    # We are unable to make much sense of this attribute.\n",
    "    df.drop('total_num_units', axis=1, inplace=True) \n",
    "\n",
    "    # Transformed year into a depreciation factor already. Drop original attribute.\n",
    "    df.drop('built_year', axis=1, inplace=True)\n",
    "\n",
    "    # Title data is non-standard and unstructured with information already available in individual attributes.\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    \n",
    "    # Property name is assumed to have no inherent effect on the value of a listing.\n",
    "    df.drop('property_name', axis=1, inplace=True)\n",
    "\n",
    "    # BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "    df.drop('furnishing', axis=1, inplace=True)\n",
    "    df.drop('available_unit_types', axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992eb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, mode='train') -> pd.DataFrame:\n",
    "    if mode=='train': \n",
    "        df = remove_invalid_values(df)\n",
    "        df = remove_outliers(df)\n",
    "    df = handle_missing_values(df, mode)\n",
    "    df = transform_data(df)\n",
    "    df = augment_data(df, distance=RADIUS_OF_INFLUENCE_KM)\n",
    "    df = ignore_attributes(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc24d1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You may skip this block if you are not interested in Section 3.\n",
    "\n",
    "df_preprocessed = pre_process(df, mode='train')\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b731a",
   "metadata": {},
   "source": [
    "## 2c. Post-process\n",
    "\n",
    "Here, we separately process the training and validation sets. Since we perform aggregation-based methods here, this is done after split to avoid information leakage and ensures salient evaluation results.\n",
    "\n",
    "If you wish to explore each model and go through our data handling, proceed to Section 3 - Model Evaluation. Alternately, if you are only interested in the final output, you may jump to Section 4 - Prediction to gather results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87847bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding_location(df, mode='train', aggregate=None):\n",
    "    # Target encode subzone based on price.\n",
    "    cols = ('subzone')\n",
    "    df.loc[:,cols] = df.loc[:,cols].fillna('null')\n",
    "    if mode == 'train':\n",
    "        aggregate = df['price'].groupby(df.loc[:,cols]).median()\n",
    "    for idx in df.index:\n",
    "        if df.at[idx, cols] in aggregate:\n",
    "            df.at[idx, cols] = float(aggregate[df.at[idx, cols]])\n",
    "        else:\n",
    "            df.at[idx, cols] = float(aggregate['null'])\n",
    "    return df, aggregate\n",
    "\n",
    "def standardize(df, mode='train', scaler=None):\n",
    "    # Normalize values here.\n",
    "    cols = ('size_sqft', 'dist_2_nearest_cc', 'dist_2_nearest_mrt', 'dist_2_nearest_ps', 'dist_2_nearest_ss', 'dist_2_nearest_sm', 'subzone')\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    if mode=='train':\n",
    "        df.loc[:,cols] = scaler.fit_transform(df.loc[:,cols])\n",
    "    elif mode=='test':\n",
    "        df.loc[:,cols] = scaler.transform(df.loc[:,cols])\n",
    "    return df, scaler\n",
    "    \n",
    "\n",
    "def post_process(df, mode='train', aggregate=None, scaler=None):\n",
    "    df, aggregate = target_encoding_location(df, mode, aggregate)\n",
    "    df, scaler = standardize(df, mode, scaler)\n",
    "    return df, aggregate, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d121ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may skip this block if you are not interested in Section 3.\n",
    "\n",
    "# First we create our train and validation split to ensure there is no information leakage from validation set to train set\n",
    "train_set, val_set = train_test_split(df_preprocessed, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True)\n",
    "\n",
    "# Then, we run post-processing on train set and use it to post-process validation set.\n",
    "train_set, aggregate, scaler = post_process(train_set, mode='train')\n",
    "val_set, _, _ = post_process(val_set, mode='test', aggregate=aggregate, scaler=scaler)\n",
    "\n",
    "# Check for why we are removing Mandai Estate!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "\n",
    "Here, we evaluate models for their predictions. RMSE is the metric used for comparison. The following models are evaluated:\n",
    "\n",
    "* Linear Regression\n",
    "* Random Forest\n",
    "* Gradient Boosting\n",
    "* AdaBoost\n",
    "* Extra Trees Regressor\n",
    "* Bagging Regressor\n",
    "* Neural Net\n",
    "\n",
    "This section also involves hyperparameter tuning wherein we search for best settings per model. Those settings are later used to set hyperparameters as in Section 1 and dictate the final predictions in Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(y, y_hat) -> None:\n",
    "    rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "    print('Validation RMSE: {:.3}'.format(rmse))\n",
    "    return\n",
    "\n",
    "# Need to recreate whole set for Section 3. This is a 'hack' to get GridSearch to work with cross-validation.\n",
    "# To ensure that we get consistent results across multiple iterations, we need to fix the train-test split.\n",
    "# Later we run GridSearch on entire set by passing in our predefined split as parameter. \n",
    "df_postprocessed = pd.concat([train_set, val_set], ignore_index=True)\n",
    "X_housing = df_postprocessed.loc[:, df_postprocessed.columns != 'price']\n",
    "y_housing = df_postprocessed['price']\n",
    "X_train = train_set.loc[:, train_set.columns != 'price']\n",
    "y_train = train_set['price']\n",
    "X_val = val_set.loc[:, val_set.columns != 'price']\n",
    "y_val = val_set['price']\n",
    "\n",
    "# Use our predefined split for GridSearch below.\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "## 3a. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = train(X_train, y_train, model_name='lr')\n",
    "y_hat_lr = predict(model_lr, X_val)\n",
    "validate(y_val, y_hat_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "## 3b. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_rf(X_feature, y_label, train_test_split):\n",
    "    estimator = RandomForestRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100, 200],\n",
    "              'max_depth': [5, 10, 20, 30, 40, 50],\n",
    "              'min_samples_split': [2, 3, 4, 5],\n",
    "              'min_samples_leaf': [1, 2, 3, 4],\n",
    "              'criterion': [\"squared_error\"],\n",
    "              'max_features': [1, 2, 3],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_rf = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_rf.fit(X_feature, y_label)\n",
    "    print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "    return model_rf\n",
    "\n",
    "model_rf = bestfit_rf(X_housing, y_housing, ps)\n",
    "y_hat_rf = predict(model_rf, X_val)\n",
    "validate(y_val, y_hat_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "## 3c. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_gb(X_feature, y_label, train_test_split):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100, 200],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'max_depth': [5, 10, 20, 30, 40, 50],\n",
    "              'min_samples_split': [2, 3, 4, 5],\n",
    "              'min_samples_leaf': [1, 2, 3, 4],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1, 2, 3],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_gb = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_gb.fit(X_feature, y_label)\n",
    "    print('Gradient Boost Best Parameters: {}'.format(model_gb.best_params_))\n",
    "    return model_gb\n",
    "    \n",
    "\n",
    "model_gb = bestfit_gb(X_housing, y_housing, ps)\n",
    "y_hat_gb = predict(model_gb, X_val)\n",
    "validate(y_val, y_hat_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "## 3d. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_ab(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "    params = {'base_estimator__max_depth': [5, 10, 20, 30, 40, 50],\n",
    "              'base_estimator__splitter': ['best', 'random'],\n",
    "              'n_estimators': [25, 50, 100, 200],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.5, 1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_ab = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_ab.fit(X_housing, y_housing)\n",
    "    print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "    return model_ab\n",
    "\n",
    "\n",
    "model_ab = bestfit_ab(X_housing, y_housing, ps)\n",
    "y_hat_ab = predict(model_ab, X_val)\n",
    "validate(y_val, y_hat_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb156ed3",
   "metadata": {},
   "source": [
    "## 3e. Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_et(X_feature, y_label, train_test_split):\n",
    "    estimator = ExtraTreesRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_et = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_et.fit(X_housing, y_housing)\n",
    "    print('Extra Trees Best Parameters: {}'.format(model_et.best_params_))\n",
    "    return model_et\n",
    "\n",
    "\n",
    "model_et = bestfit_et(X_housing, y_housing, ps)\n",
    "y_hat_et = predict(model_et, X_val)\n",
    "validate(y_val, y_hat_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9e7c3",
   "metadata": {},
   "source": [
    "## 3f. Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f889699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_br(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = BaggingRegressor(base_estimator=base_estimator)\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_br = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_br.fit(X_housing, y_housing)\n",
    "    print('Bagging Best Parameters: {}'.format(model_br.best_params_))\n",
    "    return model_br\n",
    "\n",
    "\n",
    "model_br = bestfit_br(X_housing, y_housing, ps)\n",
    "y_hat_br = predict(model_br, X_val)\n",
    "validate(y_val, y_hat_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "## 3g. Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "# 4. Prediction\n",
    "\n",
    "Now that we are done evaluating each model and comparing their performance, we select the model that performed best on the validation set to generate our overall predictions. We re-train this model from scratch on the whole set of training data here as it is assumed ready to ship so we wish to use as much data for training as we can. We later test this model against the provided test data and generate a CSV file of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train over entire training set\n",
    "df_train = pd.read_csv('data/train.csv') \n",
    "df_train = pre_process(df_train, mode='train')\n",
    "df_train, aggregate, scaler = post_process(df_train, mode='train')\n",
    "X_train = df_train.loc[:, df_train.columns != 'price']\n",
    "y_train = df_train['price']\n",
    "model = train(X_train, y_train, model_name=BEST_MODEL) # Choose your model from {'lr', 'rf', 'gb', ab', 'et', 'br', 'nn'}\n",
    "\n",
    "# Predict labels for test set\n",
    "df_test = pd.read_csv('data/test.csv') \n",
    "df_test = pre_process(df_test, mode='test')\n",
    "df_test, _, _ = post_process(df_test, mode='test', aggregate=aggregate, scaler=scaler)\n",
    "predictions = predict(model, df_test)\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.to_csv('predictions.csv', header=['Predicted'], index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ba8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
