{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba5b0",
   "metadata": {},
   "source": [
    "# Task 1: Housing Price Regression\n",
    "\n",
    "Motivation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility of notebook\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "GB_LEARNING_RATE = 0.1\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20254, 21)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Talk about pre-processing here.\n",
    "Visualize plots of original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100bab",
   "metadata": {},
   "source": [
    "For HDB,\n",
    "\n",
    "Assume that num_beds refers only to the bedrooms excluding the living rooms\n",
    "* Hdb 2-room = 1 bedroom , 1 bathroom\n",
    "* Hdb 3-room = 2 bedroom , 2 bathroom\n",
    "* Hdb 4-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 5-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 3-gen = 4 bedroom , 3 bathroom\n",
    "* Hdb Executive = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Masionette = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Jumbo = 4 bedroom, 4 bathroom\n",
    "\n",
    "References:\n",
    "* http://www.data.com.sg/template-m.jsp?p=my/1.html \n",
    "* https://www.hdb.gov.sg/residential/buying-a-flat/finding-a-flat/types-of-flats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    pass\n",
    "\n",
    "# IMPORTANT: Trivial modifications only. Do not aggregate/standardize/impute here!!\n",
    "\n",
    "def ignore_attributes(df) -> pd.DataFrame:\n",
    "    # Drop listing id; nominal identifier with no meaning\n",
    "    df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "    # Drop elevation; all the values are 0, spurious attribute\n",
    "    df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "    # Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "    df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "    # Drop floor level as 83% missing and sparse with the rest of the values. \n",
    "    # Not enough data available to get the model trained.\n",
    "    df.drop('floor_level', axis=1, inplace=True)\n",
    "\n",
    "    # Drop column property_type, tenure and furnishing as it is now encoded\n",
    "    df.drop('property_type',axis = 1, inplace=True)\n",
    "    df.drop('tenure',axis=1, inplace=True)\n",
    "    df.drop('furnishing', axis=1, inplace=True)\n",
    "\n",
    "    # BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    df.drop('address', axis=1, inplace=True)\n",
    "    df.drop('property_name', axis=1, inplace=True)\n",
    "    df.drop('built_year', axis=1, inplace=True)\n",
    "    df.drop('available_unit_types', axis=1, inplace=True)\n",
    "    df.drop('total_num_units', axis=1, inplace=True) # Bernard verified dropping it. 27.9% missing.\n",
    "    df.drop('lat', axis=1, inplace=True)\n",
    "    df.drop('lng', axis=1, inplace=True)\n",
    "    df.drop('subzone', axis=1, inplace=True)\n",
    "    df.drop('planning_area', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df) -> pd.DataFrame:\n",
    "    # Treat missing year data as new.\n",
    "    # Semantically, we define this attribute as the depreciation factor for pricing.\n",
    "    # A new house or one with missing data denotes the depreciation factor is 0 or unknown.\n",
    "    # The depreciation factor is assumed to be the difference between construction and current year.\n",
    "    # TODO: Maybe do not treat future years as current! Inflation factor might be one to look out for.\n",
    "    df['built_year'] = df['built_year'].fillna(2022)\n",
    "    \n",
    "    # TODO: 80 are missing. Should we remove them or should we keep it as 0?\n",
    "    # Verify assumption if studio qualifies as 1 bed. \n",
    "    # 75 of missing are studio, we replace the Nan as 1\n",
    "    filter_beds_studio = ((df.num_beds.isna()) & ((df.title.str.contains('studio','Studio', flags=re.IGNORECASE, regex=True))))\n",
    "    df.loc[filter_beds_studio, \"num_beds\"] = 1\n",
    "    # 5 of missing, we do not have much info. Use 0 to denote absence of attribute\n",
    "    df['num_beds'] = df['num_beds'].fillna(0)\n",
    "\n",
    "    # TODO: 400 are missing. Cannot remove so many data. Use 0 to denote absence of attribute.\n",
    "    df['num_baths'] = df['num_baths'].fillna(0)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "def handle_invalid_values(df) -> pd.DataFrame:\n",
    "    # Price is the target regression variable. If negative or 0, treat that row as invalid.\n",
    "    if 'price' in df:\n",
    "        df = df[df.price > 0]\n",
    "\n",
    "    # TODO: Verify the steps below for HDB - bed/bath ratio, price checks\n",
    "    # Filtering those with number of bathrooms more than number of bedrooms for HDB\n",
    "    filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "\n",
    "    #Filtering those with number of bathrooms more than 4, number of bedrooms more than 4 for HDB\n",
    "    filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 5)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "    \n",
    "    # price ; filtering for HDB price more than 2 million\n",
    "    if 'price' in df:\n",
    "        filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "        df = df.drop(df[filter_price_hdb].index)\n",
    "        \n",
    "    ## Outliers\n",
    "    \n",
    "    # Filtering those hdb with more than 2000 size_sqft\n",
    "    filter_size_hdb = ((df.size_sqft > 2000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df[filter_size_hdb][['property_type','num_baths','num_beds','size_sqft','price']]\n",
    "    df = df.drop(df[filter_size_hdb].index)\n",
    "    \n",
    "    \n",
    "    if 'price' in df:\n",
    "    # Filtering those data with less than $200/square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_price_sqft_200 = ((df['price per sq ft'] < 200) & (df['price per sq ft'] > 0))\n",
    "        df = df.drop(df[filter_price_sqft_200].index)\n",
    "    # Filtering those data with less than 500 square feet, and more than $5000 per square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_size = ((df['price per sq ft'] > 5000) & (df['price per sq ft'] > 0) & (df['size_sqft'] < 500))\n",
    "        df = df.drop(df[filter_size].index)\n",
    "        \n",
    "    df.drop('price per sq ft', axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def transform_data(df) -> pd.DataFrame:\n",
    "    # TODO: Test against not doing this.\n",
    "    df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "    \n",
    "    # Convert built_year into the aforementioned depreciation factor\n",
    "    df[\"depreciation\"] = (2022-df[\"built_year\"])\n",
    "    \n",
    "    # TODO: Add details on why we are doing so\n",
    "    df.loc[df.property_type.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'property_type'] = 'hdb'\n",
    "    df.loc[df.property_type.str.contains('condo', flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df['property_type'] = df['property_type'].str.lower()\n",
    "    df.loc[df.property_type.str.contains('cluster house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('townhouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('land only', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('apartment',  flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df.loc[df.property_type.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'property_type'] = 'bungalow'\n",
    "    df.loc[df.property_type.str.contains('semi-detached house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('corner terrace',flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('shophouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    df.loc[df.property_type.str.contains('conservation house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    \n",
    "    # Get one hot encoding of columns property_type\n",
    "    property_columns = ['bungalow', 'condo', 'hdb', 'corner', 'landed', 'protected', 'terraced house', 'walk-up']\n",
    "    one_hot = pd.get_dummies(df['property_type'], columns=property_columns)\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)\n",
    "    \n",
    "    df['tenure'] = df['tenure'].fillna(value=df.property_type)\n",
    "    df.loc[df.tenure.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('condo', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('terraced house', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('corner', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('landed', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('protected', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('110-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('103-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('102-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('100-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('999-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('946-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('956-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('947-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('929-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df['encoded_tenure'] = 0\n",
    "    df.loc[df.tenure.str.contains('freehold', flags=re.IGNORECASE, regex=True), 'encoded_tenure'] = 1\n",
    "    \n",
    "    df['encoded_furnishing'] = 0\n",
    "    df.loc[df.furnishing.str.contains('partial', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 0.5\n",
    "    df.loc[df.furnishing.str.contains('unfurnished', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = -1\n",
    "    df.loc[df.furnishing.str.contains('fully', flags=re.IGNORECASE, regex=True), 'encoded_furnishing'] = 1\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def pre_process(df, mode='train') -> pd.DataFrame:\n",
    "    df = handle_missing_values(df)\n",
    "    if mode=='train': df = handle_invalid_values(df)\n",
    "    df = transform_data(df)\n",
    "    df = ignore_attributes(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_preprocessed = pre_process(df, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fc24d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20060, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_beds</th>\n",
       "      <th>num_baths</th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>price</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>bungalow</th>\n",
       "      <th>condo</th>\n",
       "      <th>corner</th>\n",
       "      <th>hdb</th>\n",
       "      <th>landed</th>\n",
       "      <th>protected</th>\n",
       "      <th>terraced house</th>\n",
       "      <th>walk-up</th>\n",
       "      <th>encoded_tenure</th>\n",
       "      <th>encoded_furnishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1115</td>\n",
       "      <td>514500.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1575</td>\n",
       "      <td>995400.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>8485000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>958</td>\n",
       "      <td>2626000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732</td>\n",
       "      <td>1764000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_beds  num_baths  size_sqft      price  depreciation  bungalow  condo  \\\n",
       "0       3.0        2.0       1115   514500.0          34.0         0      0   \n",
       "1       4.0        2.0       1575   995400.0          30.0         0      0   \n",
       "2       4.0        6.0       3070  8485000.0           0.0         0      1   \n",
       "3       3.0        2.0        958  2626000.0           0.0         0      1   \n",
       "4       2.0        1.0        732  1764000.0           0.0         0      1   \n",
       "\n",
       "   corner  hdb  landed  protected  terraced house  walk-up  encoded_tenure  \\\n",
       "0       0    1       0          0               0        0               0   \n",
       "1       0    1       0          0               0        0               0   \n",
       "2       0    0       0          0               0        0               1   \n",
       "3       0    0       0          0               0        0               1   \n",
       "4       0    0       0          0               0        0               0   \n",
       "\n",
       "   encoded_furnishing  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.5  \n",
       "3                 0.5  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df_preprocessed.loc[:, df_preprocessed.columns != 'price']\n",
    "y_housing = df_preprocessed['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87847bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize/Normalize\n",
    "\n",
    "def post_process(df) -> pd.DataFrame:\n",
    "    return df\n",
    "\n",
    "\n",
    "X_train = post_process(X_train)\n",
    "X_val = post_process(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "(Add outline of steps here later)\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Extra Trees Regressor\n",
    "- Bagging Regressor\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d07e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_feature) -> pd.DataFrame: # Fix it. Numpy array is returned\n",
    "    y_hat = model.predict(X_feature)\n",
    "    return y_hat\n",
    "\n",
    "def validate(y, y_hat) -> None:\n",
    "    rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "    print('Validation RMSE: {:.3}'.format(rmse))\n",
    "    return\n",
    "\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 3.66e+06\n"
     ]
    }
   ],
   "source": [
    "def train_lr(X_feature, y_label):\n",
    "    lr = LinearRegression().fit(X_feature, y_label)\n",
    "    return lr\n",
    "\n",
    "model_lr = train_lr(X_train, y_train)\n",
    "y_hat_lr = predict(model_lr, X_val)\n",
    "validate(y_val, y_hat_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': False, 'criterion': 'squared_error', 'max_depth': 50, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 2.49e+05\n"
     ]
    }
   ],
   "source": [
    "def train_rf(X_feature, y_label):\n",
    "    rf = RandomForestRegressor(n_estimators=RF_NUM_ESTIMATORS, criterion=RF_CRITERION, max_depth=RF_MAX_DEPTH,\n",
    "                          min_samples_split=RF_MIN_SPLIT, min_samples_leaf=RF_MIN_LEAF, max_features=RF_MAX_FEATURES,\n",
    "                          bootstrap=RF_BOOTSTRAP).fit(X_feature, y_label)\n",
    "    return rf\n",
    "\n",
    "def bestfit_rf(X_feature, y_label, train_test_split):\n",
    "    estimator = RandomForestRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\"],\n",
    "              'max_features': [1],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_rf = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_rf.fit(X_feature, y_label)\n",
    "    print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "    return model_rf\n",
    "\n",
    "model_rf = bestfit_rf(X_housing, y_housing, ps)\n",
    "y_hat_rf = predict(model_rf, X_val)\n",
    "validate(y_val, y_hat_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Best Parameters: {'criterion': 'friedman_mse', 'learning_rate': 0.1, 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 2.49e+05\n"
     ]
    }
   ],
   "source": [
    "def train_gb(X_feature, y_label):\n",
    "    gb = GradientBoostingRegressor(n_estimators=GB_NUM_ESTIMATORS, learning_rate=GB_LEARNING_RATE,\n",
    "                                  max_depth=GB_MAX_DEPTH, criterion=GB_CRITERION).fit(X_feature, y_label)\n",
    "    return gb\n",
    "\n",
    "def bestfit_gb(X_feature, y_label, train_test_split):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_gb = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_gb.fit(X_feature, y_label)\n",
    "    print('Gradient Boost Best Parameters: {}'.format(model_gb.best_params_))\n",
    "    return model_gb\n",
    "    \n",
    "\n",
    "model_gb = bestfit_gb(X_housing, y_housing, ps)\n",
    "y_hat_gb = predict(model_gb, X_val)\n",
    "validate(y_val, y_hat_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 50, 'base_estimator__splitter': 'random', 'learning_rate': 0.01, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 2.75e+05\n"
     ]
    }
   ],
   "source": [
    "def train_ab():\n",
    "    pass\n",
    "\n",
    "def bestfit_ab(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "    params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "              'base_estimator__splitter': ['best', 'random'],\n",
    "              'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_ab = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_ab.fit(X_housing, y_housing)\n",
    "    print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "    return model_ab\n",
    "\n",
    "\n",
    "model_ab = bestfit_ab(X_housing, y_housing, ps)\n",
    "y_hat_ab = predict(model_ab, X_val)\n",
    "validate(y_val, y_hat_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb156ed3",
   "metadata": {},
   "source": [
    "# Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "007bf977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Best Parameters: {'criterion': 'squared_error', 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Validation RMSE: 4.71e+05\n"
     ]
    }
   ],
   "source": [
    "def train_et():\n",
    "    pass\n",
    "\n",
    "def bestfit_et(X_feature, y_label, train_test_split):\n",
    "    estimator = ExtraTreesRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_et = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_et.fit(X_housing, y_housing)\n",
    "    print('Extra Trees Best Parameters: {}'.format(model_et.best_params_))\n",
    "    return model_et\n",
    "\n",
    "\n",
    "model_et = bestfit_et(X_housing, y_housing, ps)\n",
    "y_hat_et = predict(model_et, X_val)\n",
    "validate(y_val, y_hat_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9e7c3",
   "metadata": {},
   "source": [
    "# Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f889699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Best Parameters: {'max_features': 1, 'n_estimators': 100, 'random_state': 42}\n",
      "Validation RMSE: 4.58e+06\n"
     ]
    }
   ],
   "source": [
    "def train_br():\n",
    "    pass\n",
    "\n",
    "def bestfit_br(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = BaggingRegressor(base_estimator=base_estimator)\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_br = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_br.fit(X_housing, y_housing)\n",
    "    print('Bagging Best Parameters: {}'.format(model_br.best_params_))\n",
    "    return model_br\n",
    "\n",
    "\n",
    "model_br = bestfit_br(X_housing, y_housing, ps)\n",
    "y_hat_br = predict(model_br, X_val)\n",
    "validate(y_val, y_hat_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one model here for final predictions - Linear Regression (TODO: CHANGE THIS) \n",
    "# Train over entire training set\n",
    "df_train = pd.read_csv('data/train.csv') \n",
    "df_train = pre_process(df_train, mode='train')\n",
    "X_train = df_train.loc[:, df_train.columns != 'price']\n",
    "y_train = df_train['price']\n",
    "X_train = post_process(X_train)\n",
    "model = train_rf(X_train, y_train)\n",
    "\n",
    "# Predict labels for test set\n",
    "df_test = pd.read_csv('data/test.csv') \n",
    "df_test = pre_process(df_test, mode='test')\n",
    "df_test = post_process(df_test)\n",
    "predictions = predict(model, df_test)\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.to_csv('predictions.csv', header=['Predicted'], index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65844b9",
   "metadata": {},
   "source": [
    "# Do not execute\n",
    "\n",
    "(Add misc. code here that was not utilized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37143e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
