{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba5b0",
   "metadata": {},
   "source": [
    "# Task 1: Housing Price Regression\n",
    "\n",
    "This task is focused on predicting the price of a house listing based on a pre-determined set of features. This notebook is structured as follows:\n",
    "\n",
    "1. Setup\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Model Evaluation\n",
    "4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51f1cf",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "The following section is required to setup this notebook. Import the appropriate modules and modify model hyperparameters here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility\n",
    "TRAIN_SIZE = 0.8 # Fraction of training set used for model evaluation; the remaining split is validation set.\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "GB_LEARNING_RATE = 0.1\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_SPLITTER = \"best\"\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Extra Trees Regressor: Fill in based on GridSearch results\n",
    "ET_NUM_ESTIMATORS = 100\n",
    "ET_MAX_DEPTH = 50\n",
    "ET_MAX_FEATURES = 1\n",
    "ET_MIN_SPLIT = 2\n",
    "ET_MIN_LEAF = 1\n",
    "ET_BOOTSTRAP = True\n",
    "ET_CRITERION= \"squared_error\"\n",
    "\n",
    "# Bagging Regressor: Fill in based on GridSearch results\n",
    "BR_NUM_ESTIMATORS = 100\n",
    "BR_MAX_DEPTH = 50\n",
    "BR_SPLITTER = \"best\"\n",
    "BR_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training methods here.\n",
    "\n",
    "# Linear Regression\n",
    "def train_lr(X_feature, y_label):\n",
    "    lr = LinearRegression().fit(X_feature, y_label)\n",
    "    return lr\n",
    "\n",
    "# Random Forest\n",
    "def train_rf(X_feature, y_label):\n",
    "    rf = RandomForestRegressor(n_estimators=RF_NUM_ESTIMATORS, criterion=RF_CRITERION, max_depth=RF_MAX_DEPTH,\n",
    "                          min_samples_split=RF_MIN_SPLIT, min_samples_leaf=RF_MIN_LEAF, max_features=RF_MAX_FEATURES,\n",
    "                          bootstrap=RF_BOOTSTRAP, random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    return rf\n",
    "\n",
    "# Gradient Boosting\n",
    "def train_gb(X_feature, y_label):\n",
    "    gb = GradientBoostingRegressor(n_estimators=GB_NUM_ESTIMATORS, learning_rate=GB_LEARNING_RATE,\n",
    "                                   max_depth=GB_MAX_DEPTH, criterion=GB_CRITERION, \n",
    "                                   random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    return gb\n",
    "\n",
    "# AdaBoost \n",
    "def train_ab(X_feature, y_label):\n",
    "    dt = DecisionTreeRegressor(max_depth=AB_MAX_DEPTH, splitter=AB_SPLITTER,\n",
    "                               random_state=RANDOM_CONTROL)\n",
    "    ab = AdaBoostRegressor(base_estimator=dt, n_estimators=AB_NUM_ESTIMATORS, learning_rate=AB_LEARNING_RATE,\n",
    "                          random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    return ab\n",
    "\n",
    "# Extra Trees\n",
    "def train_et(X_feature, y_label):\n",
    "    et = ExtraTreesRegressor(n_estimators=ET_NUM_ESTIMATORS, criterion=ET_CRITERION, max_depth=ET_MAX_DEPTH,\n",
    "                          min_samples_split=ET_MIN_SPLIT, min_samples_leaf=ET_MIN_LEAF, max_features=ET_MAX_FEATURES,\n",
    "                          bootstrap=ET_BOOTSTRAP, random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    return et\n",
    "\n",
    "# Bagging Regressor\n",
    "def train_br(X_feature, y_label):\n",
    "    dt = DecisionTreeRegressor(max_depth=BG_MAX_DEPTH, splitter=BG_SPLITTER,\n",
    "                               random_state=RANDOM_CONTROL)\n",
    "    br = BaggingRegressor(base_estimator=dt, n_estimators=BR_NUM_ESTIMATORS, learning_rate=BR_LEARNING_RATE,\n",
    "                         random_state=RANDOM_CONTROL).fit(X_feature, y_label)\n",
    "    return br\n",
    "\n",
    "# Neural Net\n",
    "def train_nn(X_feature, y_label):\n",
    "    pass\n",
    "\n",
    "# Generic predict method for a model.\n",
    "def predict(model, X_feature) -> pd.DataFrame: # Fix it. Numpy array is returned\n",
    "    y_hat = model.predict(X_feature)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We first explore our given training data. This is done via the following sub-tasks:\n",
    "\n",
    "2a. Visualize\n",
    "2b. Pre-process\n",
    "2c. Split\n",
    "2d. Post-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d673c87",
   "metadata": {},
   "source": [
    "## 2a. Visualize\n",
    "\n",
    "Here, we take our first look at the data to get an idea of how the attribute values are spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100bab",
   "metadata": {},
   "source": [
    "## 2b. Pre-process\n",
    "\n",
    "Here, we go through the training data and use the information gained in the section above to pick and choose what attributes we wish to consider in our analysis as well as their representation. \n",
    "\n",
    "We call this _pre-processing_ as we are working on the whole training set here. To avoid information leakage, we ensure that we do not look at the label data unless required to check for invalid inputs(more on this below). Additionally, we do not perform any aggregation or imputation here to ensure that the validation set does not influence our training set for the following sections.\n",
    "\n",
    "The _pre-processing_ is comprised of the following tasks which are performed in order:\n",
    "\n",
    "i.   Remove invalid data\n",
    "ii.  Remove outliers\n",
    "iii. Handle missing data\n",
    "iv.  Transform data\n",
    "v.   Ignore attributes\n",
    "\n",
    "TODO: Talk about each attribute here. \n",
    "\n",
    "For HDB,\n",
    "\n",
    "Assume that num_beds refers only to the bedrooms excluding the living rooms\n",
    "* Hdb 2-room = 1 bedroom , 1 bathroom\n",
    "* Hdb 3-room = 2 bedroom , 2 bathroom\n",
    "* Hdb 4-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 5-room = 3 bedroom , 2 bathroom\n",
    "* Hdb 3-gen = 4 bedroom , 3 bathroom\n",
    "* Hdb Executive = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Masionette = 3/4 bedroom, 2/3 bathroom\n",
    "* Hdb Jumbo = 4 bedroom, 4 bathroom\n",
    "\n",
    "References:\n",
    "* http://www.data.com.sg/template-m.jsp?p=my/1.html \n",
    "* https://www.hdb.gov.sg/residential/buying-a-flat/finding-a-flat/types-of-flats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_values(df) -> pd.DataFrame:\n",
    "    # Price is the target regression variable. If negative or 0, treat that row as invalid (logical assumption)\n",
    "    if 'price' in df:\n",
    "        df = df[df.price > 0]\n",
    "\n",
    "    # Filter out HDB prices more than 2,000,000 (real-world knowledge; ref. source)\n",
    "    if 'price' in df:\n",
    "        filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "        df = df.drop(df[filter_price_hdb].index)\n",
    "        \n",
    "    # Filter out HDB with more bathrooms than bedrooms (real-world knowledge; ref. source)\n",
    "    filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "\n",
    "    # Filter out HDB with more than 4 bathrooms or 6 bedrooms (real-world knowledge; ref. source)\n",
    "    filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 6)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "    \n",
    "        \n",
    "    return df\n",
    "\n",
    "def remove_outliers(df) -> pd.DataFrame:\n",
    "    # Filter out HDB with size > 2000 sqft (outlier detection)\n",
    "    filter_size_hdb = ((df.size_sqft > 2000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df[filter_size_hdb][['property_type','num_baths','num_beds','size_sqft','price']]\n",
    "    df = df.drop(df[filter_size_hdb].index)\n",
    "    \n",
    "    # Target label is used below for outlier detection. Appropriately tagged data is dropped entirely.\n",
    "    if 'price' in df:\n",
    "    # Filtering those data with less than $200/square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_price_sqft_200 = ((df['price per sq ft'] < 200) & (df['price per sq ft'] > 0))\n",
    "        df = df.drop(df[filter_price_sqft_200].index)\n",
    "    # Filtering those data with less than 500 square feet, and more than $5000 per square feet\n",
    "        df['price per sq ft'] = df['price']/df['size_sqft']\n",
    "        filter_size = ((df['price per sq ft'] > 5000) & (df['price per sq ft'] > 0) & (df['size_sqft'] < 500))\n",
    "        df = df.drop(df[filter_size].index)\n",
    "        \n",
    "        df.drop('price per sq ft', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df) -> pd.DataFrame:\n",
    "    # Treat missing year data as new.\n",
    "    # Semantically, we define this attribute as the depreciation factor for pricing.\n",
    "    # A new house or one with missing data denotes the depreciation factor is 0 or unknown.\n",
    "    # The depreciation factor is assumed to be the difference between construction and current year.\n",
    "    # TODO: Maybe do not treat future years as current! Inflation factor might be one to look out for.\n",
    "    df['built_year'] = df['built_year'].fillna(2022)\n",
    "    \n",
    "    # TODO: 80 are missing. Should we remove them or should we keep it as 0?\n",
    "    # Verify assumption if studio qualifies as 1 bed. \n",
    "    # 75 of missing are studio, we replace the Nan as 1\n",
    "    filter_beds_studio = ((df.num_beds.isna()) & ((df.title.str.contains('studio','Studio', flags=re.IGNORECASE, regex=True))))\n",
    "    df.loc[filter_beds_studio, \"num_beds\"] = 1\n",
    "    # 5 of missing, we do not have much info. Use 0 to denote absence of attribute\n",
    "    df['num_beds'] = df['num_beds'].fillna(0)\n",
    "\n",
    "    # TODO: 400 are missing. Cannot remove so many data. Use 0 to denote absence of attribute.\n",
    "    df['num_baths'] = df['num_baths'].fillna(0)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_data(df) -> pd.DataFrame:\n",
    "    # TODO: Test against not doing this.\n",
    "    df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "    \n",
    "    # Convert built_year into the aforementioned depreciation factor\n",
    "    df[\"depreciation\"] = (2022-df[\"built_year\"])\n",
    "    '''\n",
    "    # TODO: Add details on why we are doing so\n",
    "    df.loc[df.property_type.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'property_type'] = 'hdb'\n",
    "    df.loc[df.property_type.str.contains('condo', flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df['property_type'] = df['property_type'].str.lower()\n",
    "    df.loc[df.property_type.str.contains('cluster house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('townhouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('land only', flags=re.IGNORECASE, regex=True), 'property_type'] = 'landed'\n",
    "    df.loc[df.property_type.str.contains('apartment',  flags=re.IGNORECASE, regex=True), 'property_type'] = 'condo'\n",
    "    df.loc[df.property_type.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'property_type'] = 'bungalow'\n",
    "    df.loc[df.property_type.str.contains('semi-detached house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('corner terrace',flags=re.IGNORECASE, regex=True), 'property_type'] = 'corner'\n",
    "    df.loc[df.property_type.str.contains('shophouse', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    df.loc[df.property_type.str.contains('conservation house', flags=re.IGNORECASE, regex=True), 'property_type'] = 'protected'\n",
    "    \n",
    "    # Get one hot encoding of columns property_type\n",
    "    one_hot = pd.get_dummies(df['property_type'])\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)\n",
    "    '''\n",
    "    df['tenure'] = df['tenure'].fillna(value=df.property_type)\n",
    "    df.loc[df.tenure.str.contains('hdb', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('condo', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('terraced house', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('corner', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('landed', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('protected', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('bungalow', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('110-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('103-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('102-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "    df.loc[df.tenure.str.contains('100-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = '99-year leasehold'\n",
    "\n",
    "    df.loc[df.tenure.str.contains('999-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('946-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('956-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('947-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "    df.loc[df.tenure.str.contains('929-year leasehold', flags=re.IGNORECASE, regex=True), 'tenure'] = 'freehold'\n",
    "\n",
    "    df['encoded_tenure'] = 0\n",
    "    df.loc[df.tenure.str.contains('freehold', flags=re.IGNORECASE, regex=True), 'encoded_tenure'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ignore_attributes(df) -> pd.DataFrame:\n",
    "    # Drop listing id; nominal identifier with no meaning\n",
    "    df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "    # Drop elevation; all the values are 0, spurious attribute\n",
    "    df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "    # Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "    df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "    # Drop floor level as 83% missing and sparse with the rest of the values. \n",
    "    # Not enough data available to get the model trained.\n",
    "    df.drop('floor_level', axis=1, inplace=True)\n",
    "\n",
    "    # Drop column property_type and tenure as it is now encoded\n",
    "    df.drop('property_type',axis = 1, inplace=True)\n",
    "    df.drop('tenure',axis=1, inplace=True)\n",
    "\n",
    "    # BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    df.drop('address', axis=1, inplace=True)\n",
    "    df.drop('property_name', axis=1, inplace=True)\n",
    "    df.drop('built_year', axis=1, inplace=True)\n",
    "    df.drop('furnishing', axis=1, inplace=True)\n",
    "    df.drop('available_unit_types', axis=1, inplace=True)\n",
    "    df.drop('total_num_units', axis=1, inplace=True) # Bernard verified dropping it. 27.9% missing.\n",
    "    df.drop('lat', axis=1, inplace=True)\n",
    "    df.drop('lng', axis=1, inplace=True)\n",
    "    df.drop('subzone', axis=1, inplace=True)\n",
    "    df.drop('planning_area', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def pre_process(df, mode='train') -> pd.DataFrame:\n",
    "    if mode=='train': \n",
    "        df = remove_invalid_values(df)\n",
    "        df = remove_outliers(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = transform_data(df)\n",
    "    df = ignore_attributes(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc24d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pre_process(df, mode='train')\n",
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72962de1",
   "metadata": {},
   "source": [
    "## 2c. Split\n",
    "\n",
    "Here, we split the training data into training and validation sets, based on the user-defined factor from hyperparameters above. This is done solely to evaluate each model's performance as it is trained on the split set and tested against the validation set. Since it is a regression task, we use RMSE as our performance metric to compare predictions against target labels. Lower values are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df_preprocessed.loc[:, df_preprocessed.columns != 'price']\n",
    "y_housing = df_preprocessed['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e806d",
   "metadata": {},
   "source": [
    "## 2d. Post-process\n",
    "\n",
    "Here, we separately process the training and validation sets. Since we perform aggregation-based methods here, this is done after split to avoid information leakage and ensures salient evaluation results.\n",
    "\n",
    "If you wish to explore each model and go through our data handling, proceed to Section 3 - Model Evaluation. Alternately, if you are only interested in the final output, you may jump to Section 4 - Prediction to gather results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87847bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize/Normalize\n",
    "\n",
    "def post_process(df) -> pd.DataFrame:\n",
    "    return df\n",
    "\n",
    "\n",
    "X_train = post_process(X_train)\n",
    "X_val = post_process(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "\n",
    "Here, we evaluate models for their predictions. RMSE is the metric used for comparison. The following models are evaluated:\n",
    "\n",
    "3a. Linear Regression\n",
    "3b. Random Forest\n",
    "3c. Gradient Boosting\n",
    "3d. AdaBoost\n",
    "3e. Extra Trees Regressor\n",
    "3f. Bagging Regressor\n",
    "3g. Neural Net\n",
    "\n",
    "This section also involves hyperparameter tuning wherein we search for best settings per model. Those settings are later used to set hyperparameters as in Section 1 and dictate the final predictions in Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(y, y_hat) -> None:\n",
    "    rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "    print('Validation RMSE: {:.3}'.format(rmse))\n",
    "    return\n",
    "\n",
    "# Use our predefined split for GridSearch below.\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "## 3a. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = train_lr(X_train, y_train)\n",
    "y_hat_lr = predict(model_lr, X_val)\n",
    "validate(y_val, y_hat_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "## 3b. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_rf(X_feature, y_label, train_test_split):\n",
    "    estimator = RandomForestRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\"],\n",
    "              'max_features': [1],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_rf = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_rf.fit(X_feature, y_label)\n",
    "    print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "    return model_rf\n",
    "\n",
    "model_rf = bestfit_rf(X_housing, y_housing, ps)\n",
    "y_hat_rf = predict(model_rf, X_val)\n",
    "validate(y_val, y_hat_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "## 3c. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_gb(X_feature, y_label, train_test_split):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_gb = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_gb.fit(X_feature, y_label)\n",
    "    print('Gradient Boost Best Parameters: {}'.format(model_gb.best_params_))\n",
    "    return model_gb\n",
    "    \n",
    "\n",
    "model_gb = bestfit_gb(X_housing, y_housing, ps)\n",
    "y_hat_gb = predict(model_gb, X_val)\n",
    "validate(y_val, y_hat_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "## 3d. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_ab(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "    params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "              'base_estimator__splitter': ['best', 'random'],\n",
    "              'n_estimators': [25, 50, 100],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_ab = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_ab.fit(X_housing, y_housing)\n",
    "    print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "    return model_ab\n",
    "\n",
    "\n",
    "model_ab = bestfit_ab(X_housing, y_housing, ps)\n",
    "y_hat_ab = predict(model_ab, X_val)\n",
    "validate(y_val, y_hat_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb156ed3",
   "metadata": {},
   "source": [
    "## 3e. Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_et(X_feature, y_label, train_test_split):\n",
    "    estimator = ExtraTreesRegressor()\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_depth': [5, 10, 25, 50],\n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [1],\n",
    "              'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "              'max_features': [1],\n",
    "              'bootstrap': [True, False],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_et = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=train_test_split)\n",
    "    model_et.fit(X_housing, y_housing)\n",
    "    print('Extra Trees Best Parameters: {}'.format(model_et.best_params_))\n",
    "    return model_et\n",
    "\n",
    "\n",
    "model_et = bestfit_et(X_housing, y_housing, ps)\n",
    "y_hat_et = predict(model_et, X_val)\n",
    "validate(y_val, y_hat_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9e7c3",
   "metadata": {},
   "source": [
    "## 3f. Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f889699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfit_br(X_feature, y_label, train_test_split):\n",
    "    base_estimator = DecisionTreeRegressor()\n",
    "    estimator = BaggingRegressor(base_estimator=base_estimator)\n",
    "    params = {'n_estimators': [25, 50, 100],\n",
    "              'max_features': [1],\n",
    "              'random_state': [RANDOM_CONTROL]}\n",
    "    model_br = GridSearchCV(estimator=estimator,\n",
    "                         param_grid=params,\n",
    "                         cv=ps)\n",
    "    model_br.fit(X_housing, y_housing)\n",
    "    print('Bagging Best Parameters: {}'.format(model_br.best_params_))\n",
    "    return model_br\n",
    "\n",
    "\n",
    "model_br = bestfit_br(X_housing, y_housing, ps)\n",
    "y_hat_br = predict(model_br, X_val)\n",
    "validate(y_val, y_hat_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "## 3g. Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "# 4. Prediction\n",
    "\n",
    "Now that we are done evaluating each model and comparing their performance, we select the model that performed best on the validation set to generate our overall predictions. We re-train this model from scratch on the whole set of training data here as it is assumed ready to ship so we wish to use as much data for training as we can. We later test this model against the provided test data and generate a CSV file of predicted values.\n",
    "\n",
    "The following identifiers are used for each model - 'lr' for linear regression, 'rf' for random forest, 'gb' for gradient boosting, 'ab' for adaboost, 'et' for extra trees regressor, 'br' for bagging regressor, 'nn' for neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train over entire training set\n",
    "df_train = pd.read_csv('data/train.csv') \n",
    "df_train = pre_process(df_train, mode='train')\n",
    "X_train = df_train.loc[:, df_train.columns != 'price']\n",
    "y_train = df_train['price']\n",
    "X_train = post_process(X_train)\n",
    "model = train_rf(X_train, y_train) # Choose your model here. Current choice: 'rf' for Random Forest\n",
    "\n",
    "# Predict labels for test set\n",
    "df_test = pd.read_csv('data/test.csv') \n",
    "df_test = pre_process(df_test, mode='test')\n",
    "df_test = post_process(df_test)\n",
    "predictions = predict(model, df_test)\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.to_csv('predictions.csv', header=['Predicted'], index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
