{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba5b0",
   "metadata": {},
   "source": [
    "# Task 1: Housing Price Regression\n",
    "\n",
    "Motivation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility of notebook\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20254, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Talk about pre-processing here.\n",
    "Visualize plots of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    pass\n",
    "\n",
    "# IMPORTANT: Trivial modifications only. Do not aggregate/standardize/impute here!!\n",
    "\n",
    "def ignore_attributes(df) -> pd.DataFrame:\n",
    "    # Drop listing id; nominal identifier with no meaning\n",
    "    df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "    # Drop elevation; all the values are 0, spurious attribute\n",
    "    df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "    # Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "    df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "    # BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    df.drop('address', axis=1, inplace=True)\n",
    "    df.drop('property_name', axis=1, inplace=True)\n",
    "    df.drop('property_type', axis=1, inplace=True)\n",
    "    df.drop('tenure', axis=1, inplace=True)\n",
    "    df.drop('built_year', axis=1, inplace=True)\n",
    "    df.drop('floor_level', axis=1, inplace=True)\n",
    "    df.drop('furnishing', axis=1, inplace=True)\n",
    "    df.drop('available_unit_types', axis=1, inplace=True)\n",
    "    df.drop('total_num_units', axis=1, inplace=True) # Bernard verified dropping it. 27.9% missing.\n",
    "    df.drop('lat', axis=1, inplace=True)\n",
    "    df.drop('lng', axis=1, inplace=True)\n",
    "    df.drop('subzone', axis=1, inplace=True)\n",
    "    df.drop('planning_area', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df) -> pd.DataFrame:\n",
    "    # Treat missing year data as new.\n",
    "    # Semantically, we define this attribute as the depreciation factor for pricing.\n",
    "    # A new house or one with missing data denotes the depreciation factor is 0 or unknown.\n",
    "    # The depreciation factor is assumed to be the difference between construction and current year.\n",
    "    # TODO: Maybe do not treat future years as current! Inflation factor might be one to look out for.\n",
    "    df['built_year'] = df['built_year'].fillna(2022)\n",
    "    \n",
    "    # TODO: 80 are missing. Should we remove them or should we keep it as 0?\n",
    "    # Verify assumption if studio qualifies as 1 bed. All 5 NaN were studios!\n",
    "    df['num_beds'] = df['num_beds'].fillna(1) \n",
    "\n",
    "    # TODO: 400 are missing. Cannot remove so many data. Use 0 to denote absence of attribute.\n",
    "    df['num_baths'] = df['num_baths'].fillna(0)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "def handle_invalid_values(df) -> pd.DataFrame:\n",
    "    # Price is the target regression variable. If negative or 0, treat that row as invalid.\n",
    "    df = df[df.price > 0]\n",
    "\n",
    "    # TODO: Verify the steps below for HDB - bed/bath ratio, price checks\n",
    "    # Filtering those with number of bathrooms more than number of bedrooms for HDB\n",
    "    filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "\n",
    "    #Filtering those with number of bathrooms more than 4, number of bedrooms more than 4 for HDB\n",
    "    filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 5)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "    \n",
    "    # price ; filtering for HDB price more than 2 million\n",
    "    filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "    df = df.drop(df[filter_price_hdb].index)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def transform_data(df) -> pd.DataFrame:\n",
    "    # TODO: Test against not doing this.\n",
    "    df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "    \n",
    "    # Convert built_year into the aforementioned depreciation factor\n",
    "    df[\"depreciation\"] = (2022-df[\"built_year\"])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def pre_process(df) -> pd.DataFrame:\n",
    "    df = handle_missing_values(df)\n",
    "    df = handle_invalid_values(df)\n",
    "    df = transform_data(df)\n",
    "    df = ignore_attributes(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_preprocessed = pre_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc24d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20106, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_beds</th>\n",
       "      <th>num_baths</th>\n",
       "      <th>size_sqft</th>\n",
       "      <th>price</th>\n",
       "      <th>depreciation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1115</td>\n",
       "      <td>514500.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1575</td>\n",
       "      <td>995400.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>8485000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>958</td>\n",
       "      <td>2626000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>732</td>\n",
       "      <td>1764000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_beds  num_baths  size_sqft      price  depreciation\n",
       "0       3.0        2.0       1115   514500.0          34.0\n",
       "1       4.0        2.0       1575   995400.0          30.0\n",
       "2       4.0        6.0       3070  8485000.0           0.0\n",
       "3       3.0        2.0        958  2626000.0           0.0\n",
       "4       2.0        1.0        732  1764000.0           0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_preprocessed.shape)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b8afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df_preprocessed.loc[:, df_preprocessed.columns != 'price']\n",
    "y_housing = df_preprocessed['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd370a20",
   "metadata": {},
   "source": [
    "# Perform further pre-processing steps only on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "(Add outline of steps here later)\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Extra Trees Regressor\n",
    "- Bagging Regressor\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Validation rmse: 4.1e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "rmse_linreg = mean_squared_error(y_val, y_hat_linreg, squared=False)\n",
    "\n",
    "print('Linear Regression Validation rmse: {:.3}'.format(rmse_linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': True, 'criterion': 'squared_error', 'max_depth': 50, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Random Forest Validation rmse: 1.39e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\"],\n",
    "          'max_features': [1],\n",
    "          'bootstrap': [True, False],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_rf = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_rf.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_rf = model_rf.predict(X_val)\n",
    "rmse_rf = mean_squared_error(y_val, y_hat_rf, squared=False)\n",
    "\n",
    "print('Random Forest Validation rmse: {:.3}'.format(rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 10, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Gradient Boosting Validation rmse: 1.17e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "estimator = GradientBoostingRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "          'max_features': [1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_gb = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_gb.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_gb.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_gb = model_gb.predict(X_val)\n",
    "rmse_gb = mean_squared_error(y_val, y_hat_gb, squared=False)\n",
    "\n",
    "print('Gradient Boosting Validation rmse: {:.3}'.format(rmse_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 10, 'base_estimator__splitter': 'best', 'learning_rate': 0.5, 'n_estimators': 100, 'random_state': 42}\n",
      "AdaBoost Validation rmse: 1.68e+06\n"
     ]
    }
   ],
   "source": [
    "base_estimator = DecisionTreeRegressor()\n",
    "estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "          'base_estimator__splitter': ['best', 'random'],\n",
    "          'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_ab = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_ab.fit(X_housing, y_housing)\n",
    "\n",
    "print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_ab = model_ab.predict(X_val)\n",
    "rmse_ab = mean_squared_error(y_val, y_hat_ab, squared=False)\n",
    "\n",
    "print('AdaBoost Validation rmse: {:.3}'.format(rmse_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb156ed3",
   "metadata": {},
   "source": [
    "# Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "007bf977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Best Parameters: {'criterion': 'squared_error', 'max_depth': 10, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Extra Trees Validation rmse: 2.86e+06\n"
     ]
    }
   ],
   "source": [
    "estimator = ExtraTreesRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "          'max_features': [1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_et = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_et.fit(X_housing, y_housing)\n",
    "\n",
    "print('Extra Trees Best Parameters: {}'.format(model_et.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_et = model_et.predict(X_val)\n",
    "rmse_et = mean_squared_error(y_val, y_hat_et, squared=False)\n",
    "\n",
    "print('Extra Trees Validation rmse: {:.3}'.format(rmse_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9e7c3",
   "metadata": {},
   "source": [
    "# Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f889699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Best Parameters: {'max_features': 1, 'n_estimators': 100, 'random_state': 42}\n",
      "Bagging Regressor Validation rmse: 3.05e+06\n"
     ]
    }
   ],
   "source": [
    "base_estimator = DecisionTreeRegressor()\n",
    "estimator = BaggingRegressor(base_estimator=base_estimator)\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'max_features': [1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_br = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_br.fit(X_housing, y_housing)\n",
    "\n",
    "print('Bagging Best Parameters: {}'.format(model_br.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_br = model_br.predict(X_val)\n",
    "rmse_br = mean_squared_error(y_val, y_hat_br, squared=False)\n",
    "\n",
    "print('Bagging Regressor Validation rmse: {:.3}'.format(rmse_br))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "**Wrap-up**\n",
    "\n",
    "(Do this only before submitting to Kaggle; train over the entire set here after the hyperparameters are identified; then perform testing and submit results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv') \n",
    "# Pre-process similar to above. Need to refactor into a function.\n",
    "\n",
    "# Linear Regression\n",
    "linreg = LinearRegression().fit(X_housing, y_housing)\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65844b9",
   "metadata": {},
   "source": [
    "**Do not execute**\n",
    "\n",
    "(Add misc. code here that was not utilized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
