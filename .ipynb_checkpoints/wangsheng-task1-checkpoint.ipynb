{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c567016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae73599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "RANDOM_CONTROL = 42 # For reproducibility of notebook\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Random Forest: Fill in based on GridSearch results\n",
    "RF_NUM_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 50\n",
    "RF_MAX_FEATURES = 1\n",
    "RF_MIN_SPLIT = 2\n",
    "RF_MIN_LEAF = 1\n",
    "RF_BOOTSTRAP = True\n",
    "RF_CRITERION = \"squared_error\"\n",
    "\n",
    "# Gradient Boosting: Fill in based on GridSearch results\n",
    "GB_NUM_ESTIMATORS = 100\n",
    "GB_MAX_DEPTH = 50\n",
    "GB_CRITERION = \"squared_error\"\n",
    "\n",
    "# AdaBoost: Fill in based on GridSearch results\n",
    "AB_NUM_ESTIMATORS = 100\n",
    "AB_MAX_DEPTH = 50\n",
    "AB_LEARNING_RATE = 0.1\n",
    "\n",
    "# Neural Net: Fill in based on test iterations\n",
    "NN_NUM_EPOCHS = 10\n",
    "NN_BATCH_SIZE = 32\n",
    "NN_LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0064a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20254, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training data\n",
    "df = pd.read_csv('data/train.csv') \n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b8eb7",
   "metadata": {},
   "source": [
    "**EDA**\n",
    "\n",
    "(Generate plots here; preprocessing steps follow below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f278528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built_year ; replacing missing with 2022 i.e. missing treated as new\n",
    "df['built_year'] = df['built_year'].fillna(2022)\n",
    "\n",
    "# built_year ; replacing those > 2022 with 2022 ; treating the future as all new\n",
    "df.loc[df[\"built_year\"] > 2022, \"built_year\"] = 2022\n",
    "\n",
    "# built_year ; creating another parameter age instead\n",
    "df[\"age\"] = (2022-df[\"built_year\"])\n",
    "\n",
    "# Filtering those with number of bathrooms more than number of bedrooms for HDB\n",
    "filter_bath_beds_hdb = ((df.num_baths > df.num_beds) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "\n",
    "# Dropping these values which we treat as invalid\n",
    "df = df.drop(df[filter_bath_beds_hdb].index)\n",
    "\n",
    "#Filtering those with number of bathrooms more than 4, number of bedrooms more than 4 for HDB\n",
    "filter_bath_beds_4_hdb = (((df.num_baths > 4) | (df.num_beds > 5)) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "\n",
    "# Dropping these values which we treat as invalid\n",
    "df = df.drop(df[filter_bath_beds_4_hdb].index)\n",
    "\n",
    "# price ; filtering for HDB price more than 2 million\n",
    "filter_price_hdb = ((df.price > 2000000) & ((df.property_type.str.contains('hdb','Hdb', flags=re.IGNORECASE, regex=True)) | (df.title.str.contains('hdb','Hdb', regex=False))))\n",
    "\n",
    "# Dropping these values which we treat as invalid\n",
    "df = df.drop(df[filter_price_hdb].index)\n",
    "\n",
    "# Dropping total_number of units ; as 27.9% missing values\n",
    "df = df.drop(columns=['total_num_units'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2a9aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['total_num_units'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfurnishing\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailable_unit_types\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_num_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlng\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/py3913/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/py3913/lib/python3.9/site-packages/pandas/core/frame.py:4957\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4811\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4818\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4819\u001b[0m ):\n\u001b[1;32m   4820\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4821\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4822\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4955\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4956\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4963\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4964\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/py3913/lib/python3.9/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/py3913/lib/python3.9/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/py3913/lib/python3.9/site-packages/pandas/core/indexes/base.py:6661\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6662\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['total_num_units'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Trivial modifications here(DO NOT NORMALIZE/IMPUTE YET)\n",
    "\n",
    "# Drop listing id; nominal identifier with no meaning\n",
    "df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "# Drop elevation; all the values are 0, spurious attribute\n",
    "df.drop('elevation', axis=1, inplace=True)\n",
    "\n",
    "# Drop url; nominal identifier with no meaning; useful for manual lookups or scraping\n",
    "df.drop('property_details_url', axis=1, inplace=True)\n",
    "\n",
    "# Drop invalid data with negative or zero-valued price\n",
    "df = df[df.price > 0]\n",
    "\n",
    "# BELOW CODE IN THIS SECTION IS ONLY MEANT TO GET THE SKELETON WORKING; RE-EVALUATE EACH ATTRIBUTE ONE BY ONE\n",
    "df.drop('title', axis=1, inplace=True)\n",
    "df.drop('address', axis=1, inplace=True)\n",
    "# df.drop('property_name', axis=1, inplace=True)\n",
    "df.drop('property_type', axis=1, inplace=True)\n",
    "df.drop('tenure', axis=1, inplace=True)\n",
    "df.drop('built_year', axis=1, inplace=True)\n",
    "df.drop('num_beds', axis=1, inplace=True)\n",
    "df.drop('num_baths', axis=1, inplace=True)\n",
    "df.drop('floor_level', axis=1, inplace=True)\n",
    "df.drop('furnishing', axis=1, inplace=True)\n",
    "df.drop('available_unit_types', axis=1, inplace=True)\n",
    "df.drop('total_num_units', axis=1, inplace=True)\n",
    "df.drop('lat', axis=1, inplace=True)\n",
    "df.drop('lng', axis=1, inplace=True)\n",
    "df.drop('subzone', axis=1, inplace=True)\n",
    "df.drop('planning_area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc24d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20153, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b8afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055c1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation set\n",
    "\n",
    "X_housing = df.loc[:, df.columns != 'price']\n",
    "y_housing = df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_housing, y_housing, train_size=TRAIN_SIZE, random_state=RANDOM_CONTROL, shuffle=True) \n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd370a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform further pre-processing steps only on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddab579",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "(Add outline of steps here later)\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1961",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4eeedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Validation rmse: 5.85e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "rmse_linreg = mean_squared_error(y_val, y_hat_linreg, squared=False)\n",
    "\n",
    "print('Linear Regression Validation rmse: {:.3}'.format(rmse_linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43cfa1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2efd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'bootstrap': True, 'criterion': 'squared_error', 'max_depth': 25, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "Random Forest Validation rmse: 2.98e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "val_split_indices = [-1 if x in X_train.index else 0 for x in X_housing.index]\n",
    "ps = PredefinedSplit(test_fold=val_split_indices)\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\"],\n",
    "          'max_features': [1],\n",
    "          'bootstrap': [True, False],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_rf = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_rf.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_rf.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_rf = model_rf.predict(X_val)\n",
    "rmse_rf = mean_squared_error(y_val, y_hat_rf, squared=False)\n",
    "\n",
    "print('Random Forest Validation rmse: {:.3}'.format(rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c113a2",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb6b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 5, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 25, 'random_state': 42}\n",
      "Gradient Boosting Validation rmse: 3.51e+06\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "estimator = GradientBoostingRegressor()\n",
    "params = {'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'max_depth': [5, 10, 25, 50],\n",
    "          'min_samples_split': [2],\n",
    "          'min_samples_leaf': [1],\n",
    "          'criterion': [\"squared_error\", \"friedman_mse\"],\n",
    "          'max_features': [1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_gb = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_gb.fit(X_housing, y_housing)\n",
    "\n",
    "print('Random Forest Best Parameters: {}'.format(model_gb.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_gb = model_gb.predict(X_val)\n",
    "rmse_gb = mean_squared_error(y_val, y_hat_gb, squared=False)\n",
    "\n",
    "print('Gradient Boosting Validation rmse: {:.3}'.format(rmse_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca5f8e",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7cf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'base_estimator__max_depth': 50, 'base_estimator__splitter': 'best', 'learning_rate': 1, 'n_estimators': 25, 'random_state': 42}\n",
      "AdaBoost Validation rmse: 2.97e+06\n"
     ]
    }
   ],
   "source": [
    "base_estimator = DecisionTreeRegressor()\n",
    "estimator = AdaBoostRegressor(base_estimator=base_estimator)\n",
    "params = {'base_estimator__max_depth': [5, 10, 25, 50],\n",
    "          'base_estimator__splitter': ['best', 'random'],\n",
    "          'n_estimators': [25, 50, 100],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
    "          'random_state': [RANDOM_CONTROL]}\n",
    "model_ab = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=params,\n",
    "                     cv=ps)\n",
    "model_ab.fit(X_housing, y_housing)\n",
    "\n",
    "print('AdaBoost Best Parameters: {}'.format(model_ab.best_params_))\n",
    "\n",
    "# Validate\n",
    "\n",
    "y_hat_ab = model_ab.predict(X_val)\n",
    "rmse_ab = mean_squared_error(y_val, y_hat_ab, squared=False)\n",
    "\n",
    "print('AdaBoost Validation rmse: {:.3}'.format(rmse_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32351",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d50be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing = X_housing.to_numpy()\n",
    "y_housing = y_housing.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "train_dataloader = DataLoader([ [X_train[i], y_train[i]] for i in range(len(X_train)) ], batch_size=NN_BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba23f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanata/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 Batches processed | 99] Loss: 74088216.0000\n",
      "Epoch [1 Batches processed | 199] Loss: 3812634.0000\n",
      "Epoch [1 Batches processed | 299] Loss: 4383063.5000\n",
      "Epoch [1 Batches processed | 399] Loss: 13035016.0000\n",
      "Epoch [1 Batches processed | 499] Loss: 3967107.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanata/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2 Batches processed | 99] Loss: 4189813.0000\n",
      "Epoch [2 Batches processed | 199] Loss: 3987484.7500\n",
      "Epoch [2 Batches processed | 299] Loss: 4095743.7500\n",
      "Epoch [2 Batches processed | 399] Loss: 82362232.0000\n",
      "Epoch [2 Batches processed | 499] Loss: 4018017.5000\n",
      "Epoch [3 Batches processed | 99] Loss: 3835343.0000\n",
      "Epoch [3 Batches processed | 199] Loss: 82014840.0000\n",
      "Epoch [3 Batches processed | 299] Loss: 3885355.5000\n",
      "Epoch [3 Batches processed | 399] Loss: 4321278.5000\n",
      "Epoch [3 Batches processed | 499] Loss: 4217107.0000\n",
      "Epoch [4 Batches processed | 99] Loss: 4044569.2500\n",
      "Epoch [4 Batches processed | 199] Loss: 73373376.0000\n",
      "Epoch [4 Batches processed | 299] Loss: 4110977.2500\n",
      "Epoch [4 Batches processed | 399] Loss: 4052472.2500\n",
      "Epoch [4 Batches processed | 499] Loss: 13030081.0000\n",
      "Epoch [5 Batches processed | 99] Loss: 4105910.7500\n",
      "Epoch [5 Batches processed | 199] Loss: 73418200.0000\n",
      "Epoch [5 Batches processed | 299] Loss: 3910133.7500\n",
      "Epoch [5 Batches processed | 399] Loss: 4246490.0000\n",
      "Epoch [5 Batches processed | 499] Loss: 13013023.0000\n",
      "Epoch [6 Batches processed | 99] Loss: 82437800.0000\n",
      "Epoch [6 Batches processed | 199] Loss: 4220987.0000\n",
      "Epoch [6 Batches processed | 299] Loss: 3851550.0000\n",
      "Epoch [6 Batches processed | 399] Loss: 4288211.0000\n",
      "Epoch [6 Batches processed | 499] Loss: 3989931.0000\n",
      "Epoch [7 Batches processed | 99] Loss: 3861683.0000\n",
      "Epoch [7 Batches processed | 199] Loss: 4397511.5000\n",
      "Epoch [7 Batches processed | 299] Loss: 3927601.2500\n",
      "Epoch [7 Batches processed | 399] Loss: 12824812.0000\n",
      "Epoch [7 Batches processed | 499] Loss: 73668688.0000\n",
      "Epoch [8 Batches processed | 99] Loss: 4424376.0000\n",
      "Epoch [8 Batches processed | 199] Loss: 4382395.5000\n",
      "Epoch [8 Batches processed | 299] Loss: 3866507.5000\n",
      "Epoch [8 Batches processed | 399] Loss: 73210584.0000\n",
      "Epoch [8 Batches processed | 499] Loss: 12925530.0000\n",
      "Epoch [9 Batches processed | 99] Loss: 4385586.0000\n",
      "Epoch [9 Batches processed | 199] Loss: 3906684.2500\n",
      "Epoch [9 Batches processed | 299] Loss: 73523296.0000\n",
      "Epoch [9 Batches processed | 399] Loss: 12865217.0000\n",
      "Epoch [9 Batches processed | 499] Loss: 4206032.5000\n",
      "Epoch [10 Batches processed | 99] Loss: 12781725.0000\n",
      "Epoch [10 Batches processed | 199] Loss: 73639928.0000\n",
      "Epoch [10 Batches processed | 299] Loss: 4634209.5000\n",
      "Epoch [10 Batches processed | 399] Loss: 3830202.2500\n",
      "Epoch [10 Batches processed | 499] Loss: 3886769.0000\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# Define MLP architecture\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Modify accordingly\n",
    "        self.fc1 = nn.Linear(1, 4)\n",
    "        self.fc2 = nn.Linear(4, 16)\n",
    "        self.fc3 = nn.Linear(16, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.fc5 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dense = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3, self.relu, \n",
    "                                   self.fc4, self.relu, self.fc5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.dense(x)\n",
    "        return pred\n",
    "    \n",
    "# Train\n",
    "device = 'cpu'\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), NN_LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for epoch in range(NN_NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for idx, (x_features, y_labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_features = x_features.to(device, dtype=torch.float)\n",
    "        y_labels = y_labels.to(device, dtype=torch.float)\n",
    "        prediction = model(x_features)\n",
    "        loss = torch.sqrt(criterion(prediction, y_labels)) # Standardize RMSE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        if (idx+1) %100 == 0: \n",
    "            running_loss = format(running_loss/100, '.4f')\n",
    "            print(f\"Epoch [{epoch+1} Batches processed | {idx}] Loss: {running_loss}\")\n",
    "            running_loss = 0\n",
    "print(\"Finished Training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "X_features = torch.from_numpy(X_val)\n",
    "y_labels = torch.from_numpy(y_val)\n",
    "X_features = X_features.to(device, dtype=torch.float)\n",
    "y_labels = y_labels.to(device, dtype=torch.float)\n",
    "prediction = model(X_features)\n",
    "rmse_nn = torch.sqrt(criterion(prediction, y_labels))\n",
    "\n",
    "print('Neural Net Validation rmse: {:.3}'.format(rmse_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399c06a",
   "metadata": {},
   "source": [
    "**Wrap-up**\n",
    "\n",
    "(Do this only before submitting to Kaggle; train over the entire set here after the hyperparameters are identified; then perform testing and submit results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv') \n",
    "# Pre-process similar to above. Need to refactor into a function.\n",
    "\n",
    "# Linear Regression\n",
    "linreg = LinearRegression().fit(X_housing, y_housing)\n",
    "y_hat_linreg = linreg.predict(X_val)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65844b9",
   "metadata": {},
   "source": [
    "**Do not execute**\n",
    "\n",
    "(Add misc. code here that was not utilized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
